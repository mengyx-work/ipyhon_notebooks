{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "sys.path.append('/home/ymm/kaggle/xgboost_hyperopt')\n",
    "import utils.bosch_functions as bosch_functions\n",
    "from utils.wrapped_xgboost import xgboost_classifier\n",
    "from utils.validation_tools import score_MCC, MCC, create_validation_index\n",
    "from utils.models import CombinedModel\n",
    "from utils.data_munge import remove_single_value_columns\n",
    "\n",
    "data_path = '/home/ymm/bosch/'\n",
    "\n",
    "train_num_file   = 'train_numeric.csv'\n",
    "train_cat_file   = 'train_categorical.csv'\n",
    "train_date_file  = 'train_date.csv'\n",
    "test_num_file    = 'test_numeric.csv'\n",
    "test_cat_file    = 'test_categorical.csv'\n",
    "test_date_file   = 'test_date.csv'\n",
    "\n",
    "sample_submission_file   = 'sample_submission.csv'\n",
    "\n",
    "\n",
    "start_time_column_name = 'L0_S0_D1'\n",
    "id_column_name = 'Id'\n",
    "dep_var_name = 'Response'\n",
    "nan_fill_value = -2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section to select data (rows-wise) based on the start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading takes  57.7  seconds.\n"
     ]
    }
   ],
   "source": [
    "bin_num = 1 ## number of bins to separate data by start_time\n",
    "tmp_train, tmp_test, bins, bin_names = bosch_functions.create_grouped_index_df(bin_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def creat_non_selected_window_num(bin_num, select_bin = '0'):\n",
    "    none_selected_window_num = [np.NaN]\n",
    "    for i in range(bin_num):\n",
    "        if str(i) != select_bin:\n",
    "            none_selected_window_num.append(str(i))\n",
    "    return none_selected_window_num\n",
    "    \n",
    "none_selected_window_num = creat_non_selected_window_num(bin_num, '0')\n",
    "## select NaN data only\n",
    "none_selected_window_num = ['0']\n",
    "skipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "skipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183747, 3) (1183748, 3) 673861 674503\n"
     ]
    }
   ],
   "source": [
    "print tmp_train.shape, tmp_test.shape, len(skipped_train_row_num), len(skipped_test_row_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading date using 49.0 seconds\n"
     ]
    }
   ],
   "source": [
    "nrows = 100000\n",
    "## select all the numerical columns and try to use LR\n",
    "start_time = time.time()\n",
    "train_num = pd.read_csv(join(data_path, train_num_file),  skiprows=skipped_train_row_num,  index_col='Id', nrows = nrows)\n",
    "train_dat = pd.read_csv(join(data_path, train_date_file), skiprows=skipped_train_row_num,  index_col='Id', nrows = nrows)\n",
    "train_cat = pd.read_csv(join(data_path, train_cat_file),  skiprows=skipped_train_row_num,  index_col='Id', nrows = nrows)\n",
    "print 'finish loading date using {} seconds'.format(round(time.time() - start_time, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the regular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnrows = 50000\\n## select all the numerical columns and try to use LR\\nstart_time = time.time()\\ntrain_num = pd.read_csv(join(data_path, train_num_file),    index_col='Id', nrows = nrows)\\ntrain_dat = pd.read_csv(join(data_path, train_date_file),   index_col='Id', nrows = nrows)\\ntrain_cat = pd.read_csv(join(data_path, train_cat_file),    index_col='Id', nrows = nrows)\\nprint 'finish loading date using {} seconds'.format(round(time.time() - start_time, 0))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nrows = 50000\n",
    "## select all the numerical columns and try to use LR\n",
    "start_time = time.time()\n",
    "train_num = pd.read_csv(join(data_path, train_num_file),    index_col='Id', nrows = nrows)\n",
    "train_dat = pd.read_csv(join(data_path, train_date_file),   index_col='Id', nrows = nrows)\n",
    "train_cat = pd.read_csv(join(data_path, train_cat_file),    index_col='Id', nrows = nrows)\n",
    "print 'finish loading date using {} seconds'.format(round(time.time() - start_time, 0))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1156) (100000, 969) (100000, 2140)\n"
     ]
    }
   ],
   "source": [
    "print train_dat.shape, train_num.shape, train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_train_num = train_num\n",
    "tmp_train_dat = train_dat\n",
    "tmp_train_cat = train_cat\n",
    "#tmp_train_num = train_num.copy()\n",
    "#tmp_train_dat = train_dat.copy()\n",
    "#tmp_train_cat = train_cat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stat processing categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def BasicCat_FeatureEngineering(train_cat):\n",
    "    ## feature engineering on the date features\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    column_names = train_cat.columns.tolist()\n",
    "    column_names.append('NaN')\n",
    "    encoder.fit(column_names)\n",
    "    dat_new_fea = pd.DataFrame()\n",
    "    dat_new_fea['cat_sum'] = train_cat.sum(axis=1)\n",
    "    dat_new_fea['cat_mean'] = train_cat.mean(axis=1)\n",
    "    dat_new_fea['cat_nan_count'] = train_cat.isnull().sum(axis=1)\n",
    "    dat_new_fea['cat_max'] = train_cat.max(axis=1)\n",
    "    dat_new_fea['cat_min'] = train_cat.min(axis=1)\n",
    "    dat_new_fea['cat_max_min_diff'] = dat_new_fea['cat_max'] - dat_new_fea['cat_min']\n",
    "    dat_new_fea['cat_max_min_ratio'] = dat_new_fea['cat_min'] / dat_new_fea['cat_max']\n",
    "\n",
    "    dat_new_fea['cat_idxmax'] = train_cat.idxmax(axis=1)\n",
    "    dat_new_fea['cag_idxmax'].fillna('NaN', inplace=True)\n",
    "    dat_new_fea['cat_idxmax'] = encoder.transform(dat_new_fea['cat_idxmax'])\n",
    "    dat_new_fea['cat_idxmin'] = train_cat.idxmin(axis=1)\n",
    "    dat_new_fea['cat_idxmin'].fillna('NaN', inplace=True)\n",
    "    dat_new_fea['cat_idxmin'] = encoder.transform(dat_new_fea['cat_idxmin'])\n",
    "    return dat_new_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train data dimension:  (100000, 2140)\n",
      "processed train data dimension:  (100000, 1333)\n",
      "raw train data dimension:  (100000, 969)\n",
      "processed train data dimension:  (100000, 957)\n",
      "raw train data dimension:  (100000, 1156)\n",
      "processed train data dimension:  (100000, 1142)\n"
     ]
    }
   ],
   "source": [
    "remove_single_value_columns(tmp_train_cat)\n",
    "remove_single_value_columns(tmp_train_num)\n",
    "remove_single_value_columns(tmp_train_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### collect all the unique levels from categorical features and transform data by each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_value_set = set()\n",
    "for col in tmp_train_cat.columns:\n",
    "    uniques = tmp_train_cat[col].dropna().unique()\n",
    "    unique_value_set |= set(uniques)\n",
    "    \n",
    "level_mapping_dict = {}\n",
    "for i, value in enumerate(sorted(list(unique_value_set))):\n",
    "    level_mapping_dict[value] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_level_list = list(unique_value_set)\n",
    "sorted_level_list = sorted(sorted_level_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish feature engineering date using 26.16 minutes\n"
     ]
    }
   ],
   "source": [
    "## generic function to encode categorical features\n",
    "def sweep_up_categorical_encode_by_dep_var(df, fea_name, test_df = None, dep_var_name='Response', count_thres = 10, nan_fill = -1., const_scale = 1.):\n",
    "    tmp_df = df[[fea_name, dep_var_name]]\n",
    "    tmp_df = tmp_df.fillna(nan_fill)\n",
    "    value_counts = tmp_df[fea_name].value_counts()\n",
    "    minor_keys = []\n",
    "    key_dep_var_map = {}\n",
    "    \n",
    "    ## training sweep-up\n",
    "    for counts, key in zip(value_counts.values, value_counts.index):\n",
    "        if counts > count_thres:\n",
    "            mean_dep_var = const_scale * tmp_df.loc[tmp_df[fea_name] == key, dep_var_name].mean()\n",
    "            key_dep_var_map[key] = mean_dep_var\n",
    "        else:\n",
    "            minor_keys.append(key)\n",
    "            \n",
    "    ## mean value of dep_var for all the minor levels\n",
    "    if len(minor_keys) > 0:\n",
    "        minor_key_dep_var_mean = const_scale * tmp_df.loc[tmp_df[fea_name].isin(minor_keys), dep_var_name].mean()\n",
    "        ## update the key_dep_var_map with minor key\n",
    "        for key in minor_keys:\n",
    "            key_dep_var_map[key] = minor_key_dep_var_mean\n",
    "    \n",
    "    encoded_train = tmp_df[fea_name].replace(key_dep_var_map)\n",
    "    overall_mean_dep_var = tmp_df[dep_var_name].mean()\n",
    "    \n",
    "    ## sweep up the test column\n",
    "    if test_df is not None:\n",
    "        test_value_counts = test_df[fea_name].value_counts()\n",
    "        test_minor_keys = []\n",
    "        test_key_dep_var_map = key_dep_var_map.copy()\n",
    "    \n",
    "        for counts, key in zip(test_value_counts.values, test_value_counts.index):\n",
    "            if key not in test_key_dep_var_map:\n",
    "                print 'new level {} with counts {} found in test data'.format(key, counts)\n",
    "                if counts > count_thres:\n",
    "                    print 'warning! new level {} is found in test data!'.format(key)\n",
    "                else:\n",
    "                    test_minor_keys.append(key)\n",
    "        \n",
    "        if len(test_minor_keys) > 0:\n",
    "            for key in test_minor_keys:\n",
    "                test_key_dep_var_map[key] = const_scale * overall_mean_dep_var\n",
    "        \n",
    "        encoded_test = test_df[fea_name].replace(test_key_dep_var_map)\n",
    "        return encoded_train, encoded_test\n",
    "    \n",
    "    else:\n",
    "        return encoded_train\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def getCat_LevelFeatures(series):\n",
    "    feature_array = []\n",
    "    tmp_series = series.dropna()\n",
    "    for level in sorted_level_list:\n",
    "        if sum(tmp_series.isin([level])):\n",
    "            level_index = tmp_series[tmp_series == level].index\n",
    "            feature_array.extend([level_index[0], level_index[-1], len(level_index)])\n",
    "        else:\n",
    "            feature_array.extend(['NaN', 'NaN', 0])\n",
    "    return pd.Series(feature_array)       \n",
    "    #print series.value_counts()       \n",
    "    value_counts = series.value_counts()\n",
    "\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "levelFeatures = tmp_train_cat.apply(getCat_LevelFeatures, axis=1)\n",
    "\n",
    "print 'finish feature engineering date using {} minutes'.format(round((time.time() - start_time)/60., 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sorted_level_list\n",
    "column_name_features = []\n",
    "cat_fea_names = []\n",
    "for level in sorted_level_list:\n",
    "    cat_fea_names.extend(['level_{}_start_column'.format(level), \n",
    "                          'level_{}_end_column'.format(level), \n",
    "                          'level_{}_column_counts'.format(level)])\n",
    "    \n",
    "    column_name_features.extend(['level_{}_start_column'.format(level), \n",
    "                                 'level_{}_end_column'.format(level)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levelFeatures.columns = cat_fea_names\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "column_names = train_cat.columns.tolist()\n",
    "column_names.append('NaN')\n",
    "encoder.fit(column_names)\n",
    "\n",
    "for col in column_name_features:\n",
    "    levelFeatures[col] = encoder.transform(levelFeatures[col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1333)\n",
      "(100000, 1327)\n"
     ]
    }
   ],
   "source": [
    "## One-Hot encode important categorical columns\n",
    "print tmp_train_cat.shape\n",
    "imp_cat_fea = ['L3_S29_F3317', 'L3_S35_F3907', 'L3_S49_F4217', 'L3_S35_F3902', 'L3_S32_F3851', 'L3_S32_F3854']\n",
    "cat_fea = []\n",
    "for col in imp_cat_fea:\n",
    "    if col in tmp_train_cat.columns:\n",
    "        cat_fea.append(col)\n",
    "\n",
    "imp_cat_df = tmp_train_cat[cat_fea].astype(str)\n",
    "oneHot_combined_cat = pd.get_dummies(imp_cat_df, dummy_na=True)\n",
    "tmp_train_cat = tmp_train_cat.ix[:, ~tmp_train_cat.columns.isin(imp_cat_fea)]\n",
    "print tmp_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 44)\n"
     ]
    }
   ],
   "source": [
    "print oneHot_combined_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint levelFeatures.shape\\nlevelFeatures.head()\\n\\nprint tmp_train_cat.shape\\ntmp_train_cat.head()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print levelFeatures.shape\n",
    "levelFeatures.head()\n",
    "\n",
    "print tmp_train_cat.shape\n",
    "tmp_train_cat.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_train_cat = pd.concat([tmp_train_cat, oneHot_combined_cat, levelFeatures], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1533)\n"
     ]
    }
   ],
   "source": [
    "print combined_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### functions to process numerical and date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BasicDate_FeatureEngineering(tmp_train_dat):\n",
    "    ## feature engineering on the date features\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    column_names = tmp_train_dat.columns.tolist()\n",
    "    column_names.append('NaN')\n",
    "    encoder.fit(column_names)\n",
    "    dat_new_fea = pd.DataFrame()\n",
    "    \n",
    "    if 'L0_S0_D1' in tmp_train_dat.columns:\n",
    "        dat_new_fea['start_time'] = tmp_train_dat['L0_S0_D1']\n",
    "        \n",
    "    dat_new_fea['time_sum'] = tmp_train_dat.sum(axis=1)\n",
    "    dat_new_fea['time_mean'] = tmp_train_dat.mean(axis=1)\n",
    "    dat_new_fea['dat_nan_count'] = tmp_train_dat.isnull().sum(axis=1)\n",
    "    dat_new_fea['max_time'] = tmp_train_dat.max(axis=1)\n",
    "    dat_new_fea['min_time'] = tmp_train_dat.min(axis=1)\n",
    "    dat_new_fea['dat_max_min_diff'] = dat_new_fea['max_time'] - dat_new_fea['min_time']\n",
    "    dat_new_fea['dat_max_min_ratio'] = dat_new_fea['min_time'] / dat_new_fea['max_time']\n",
    "\n",
    "    dat_new_fea['dat_idxmax'] = tmp_train_dat.idxmax(axis=1)\n",
    "    dat_new_fea['dat_idxmax'].fillna('NaN', inplace=True)\n",
    "    dat_new_fea['dat_idxmax'] = encoder.transform(dat_new_fea['dat_idxmax'])\n",
    "    dat_new_fea['dat_idxmin'] = tmp_train_dat.idxmin(axis=1)\n",
    "    dat_new_fea['dat_idxmin'].fillna('NaN', inplace=True)\n",
    "    dat_new_fea['dat_idxmin'] = encoder.transform(dat_new_fea['dat_idxmin'])\n",
    "    return dat_new_fea\n",
    "\n",
    "\n",
    "def getRelativeTimeColumns(series):\n",
    "    '''\n",
    "    normalize the time features by\n",
    "    the start_time, the first none-NaN\n",
    "    value\n",
    "    '''\n",
    "    if series[0] == np.NaN:\n",
    "        start_time = series.dropna().index[0]\n",
    "    else:\n",
    "        start_time = series[0]\n",
    "    new_series = series - start_time\n",
    "    return new_series\n",
    "   \n",
    "\n",
    "\n",
    "def getTimeSteps(series, unique_value_counts = 10):\n",
    "    '''\n",
    "    in each row/series, use the sorted value_count\n",
    "    to find the time steps and use the value, counts\n",
    "    and column_index as features\n",
    "    '''\n",
    "    value_counts = series.value_counts()\n",
    "    value_counts.sort_index(inplace=True)\n",
    "\n",
    "    if 0. in value_counts.index:\n",
    "        value_counts = value_counts[value_counts.index != 0.]\n",
    "        \n",
    "    available_counts = value_counts.shape[0]\n",
    "    feature_array = []\n",
    "    for i in xrange(unique_value_counts):\n",
    "        if i < available_counts:\n",
    "            date_value = value_counts.index[i]\n",
    "            counts = value_counts[date_value]\n",
    "            first_index = series[series == date_value].index[0]\n",
    "            avg_time_cost = date_value / counts\n",
    "            feature = [date_value, counts, avg_time_cost, first_index]\n",
    "        else:\n",
    "            feature = [np.NaN, 0, 0, 'NaN']\n",
    "        feature_array.extend(feature)\n",
    "\n",
    "    return pd.Series(feature_array)\n",
    "\n",
    "\n",
    "\n",
    "def getTimeChangeColumns(series):\n",
    "    start_time = series[0]\n",
    "    tmp_series = series.dropna()\n",
    "    if start_time == np.NaN:\n",
    "        first_index     = tmp_series.index[0]\n",
    "        last_index      = tmp_series.index[-1]\n",
    "        first_id_value  = tmp_series[first_index]\n",
    "        last_id_value   = tmp_series[last_index]\n",
    "        first_num_value = first_id_value\n",
    "        time_diff       = last_id_value - first_id_value\n",
    "        time_ratio      = last_id_value / first_id_value\n",
    "        return pd.Series([first_index, last_index, time_diff, time_ratio, \n",
    "                          first_id_value, last_id_value, first_num_value])\n",
    "    else:\n",
    "        first_num_value = start_time\n",
    "        if np.sum(tmp_series != start_time) == 0:\n",
    "            return pd.Series(['NaN', 'NaN', np.NaN, np.NaN, np.NaN, np.NaN, first_num_value])\n",
    "        else:\n",
    "            first_index     = tmp_series.index[tmp_series != start_time][0]\n",
    "            last_index      = tmp_series.index[tmp_series != start_time][-1]\n",
    "            first_id_value  = series[first_index]\n",
    "            last_id_value   = series[last_index]\n",
    "            first_id_value  = tmp_series[first_index]\n",
    "            last_id_value   = tmp_series[last_index]\n",
    "            time_diff       = last_id_value - first_id_value\n",
    "            time_ratio      = last_id_value / first_id_value\n",
    "\n",
    "            return pd.Series([first_index, last_index, time_diff, time_ratio,\n",
    "                              first_id_value, last_id_value, first_num_value])\n",
    "\n",
    "\n",
    "        \n",
    "def getNonNaN_ColumIndex(series):\n",
    "    if series.notnull().sum() == 0:\n",
    "        return pd.Series([np.NaN, np.NaN, np.NaN])\n",
    "    else:\n",
    "        first_id = series[series.notnull()].index[0]\n",
    "        last_id = series[series.notnull()].index[-1]\n",
    "        time_diff = series[last_id] - series[first_id]\n",
    "        return pd.Series([first_id, last_id, time_diff, first_time_value, last_time_value])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def DateFeatureEngineering(series):\n",
    "    '''\n",
    "    combination of time difference features and none-NaN features    \n",
    "    this method is faster than separating two functions\n",
    "    '''\n",
    "    ## time features\n",
    "    start_time = series[0]\n",
    "    tmp_series = series.dropna()\n",
    "    if start_time == np.NaN:\n",
    "        first_index = tmp_series.index[0]\n",
    "        last_index  = tmp_series.index[-1]\n",
    "        time_diff   = tmp_series[last_index] - tmp_series[first_index]\n",
    "        time_fea    = np.array([first_index, last_index, time_diff])\n",
    "    else:\n",
    "        if np.sum(tmp_series != start_time) == 0:\n",
    "            time_fea = np.array([np.NaN, np.NaN, np.NaN])\n",
    "        else:\n",
    "            first_index = tmp_series.index[tmp_series != start_time][0]\n",
    "            last_index  = tmp_series.index[tmp_series != start_time][-1]\n",
    "            time_diff   = tmp_series[last_index] - tmp_series[first_index]\n",
    "            time_fea = np.array([first_index, last_index, time_diff])\n",
    "    ## none-NaN features\n",
    "    if series.notnull().sum() == 0:\n",
    "        nan_fea = np.array([np.NaN, np.NaN, np.NaN])\n",
    "    else:\n",
    "        first_id  = tmp_series.index[0]\n",
    "        last_id   = tmp_series.index[-1]\n",
    "        time_diff = tmp_series[last_id] - tmp_series[first_id]\n",
    "        nan_fea = np.array([first_id, last_id, time_diff])\n",
    "         \n",
    "    new_fea_row = np.concatenate([time_fea, nan_fea])\n",
    "    return pd.Series(new_fea_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start processing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def NumericalFeatureEngineering(df, col_ignore = ['Response']):\n",
    "    tmp_df = df.loc[:, ~df.columns.isin(col_ignore)]\n",
    "    new_fea_df = pd.DataFrame()\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    column_names = tmp_df.columns.tolist()\n",
    "    column_names.append('NaN')\n",
    "    encoder.fit(column_names)\n",
    "    \n",
    "    new_fea_df['num_mean'] = tmp_df.mean(axis=1)\n",
    "    \n",
    "    new_fea_df['num_sum'] = tmp_df.sum(axis=1)\n",
    "    #num_sum_max = new_fea_df['num_sum'].max()\n",
    "    #new_fea_df['num_sum'].fillna(1.*int(num_sum_max - 5), inplace=True)\n",
    "    new_fea_df['num_max'] = tmp_df.max(axis=1)\n",
    "    #num_max_max = new_fea_df['num_max'].max()\n",
    "    #new_fea_df['num_max'].fillna(1.*int(num_max_max + 1), inplace=True)\n",
    "    new_fea_df['num_min'] = tmp_df.min(axis=1)\n",
    "    #num_min_min = new_fea_df['num_min'].min()\n",
    "    #new_fea_df['num_min'].fillna(1.*int(num_min_min - 1.), inplace=True)\n",
    "    new_fea_df['num_max_min_ratio'] = new_fea_df['num_min'] / new_fea_df['num_max']\n",
    "    new_fea_df['num_max_min_ratio'] = new_fea_df['num_max_min_ratio'].replace([np.inf, -np.inf], np.NaN)\n",
    "    new_fea_df['num_nan_col_count'] = tmp_df.isnull().sum(axis=1)\n",
    "    new_fea_df['num_reg_col_count'] = tmp_df.shape[1] - tmp_df.isnull().sum(axis=1)\n",
    "    new_fea_df['num_idxmax'] = tmp_df.idxmax(axis=1)\n",
    "    new_fea_df['num_idxmax'].fillna('NaN', inplace=True)\n",
    "    new_fea_df['num_idxmax'] = encoder.transform(new_fea_df['num_idxmax'])\n",
    "    new_fea_df['num_idxmin'] = tmp_df.idxmin(axis=1)\n",
    "    new_fea_df['num_idxmin'].fillna('NaN', inplace=True)\n",
    "    new_fea_df['num_idxmin'] = encoder.transform(new_fea_df['num_idxmin'])\n",
    "    return new_fea_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_train_num_Basics = NumericalFeatureEngineering(tmp_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print tmp_train_num_Basics.min().min(), tmp_train_num_Basics.max().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill up the NaN in numerical features with nan_fill_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print tmp_train_num.max().max(), '\\n', tmp_train_num.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tmp_train_num.fillna(nan_fill_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_train_num = pd.concat([tmp_train_num, tmp_train_num_Basics], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print combined_train_num.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start processing date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish feature engineering date using 4.27 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "## normalized date columns\n",
    "tmp_train_dat_Norm = tmp_train_dat.apply(getRelativeTimeColumns, axis=1)\n",
    "## basic features from tmp_train_dat\n",
    "tmp_train_dat_Basics = BasicDate_FeatureEngineering(tmp_train_dat)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "column_names = tmp_train_dat.columns.tolist()\n",
    "column_names.append('NaN')\n",
    "encoder.fit(column_names)\n",
    "    \n",
    "#'''\n",
    "tmp_train_dat_TimeDiff = tmp_train_dat.apply(getTimeChangeColumns, axis=1)\n",
    "tmp_train_dat_TimeDiff.columns = ['time_diff_start_col', 'time_diff_end_col', 'time_diff_value', \n",
    "                                  'time_ratio_value', 'first_time_value', 'last_time_value', 'first_date_value']\n",
    "                   \n",
    "for column in ['time_diff_start_col', 'time_diff_end_col']:\n",
    "    tmp_train_dat_TimeDiff[column].fillna('NaN', inplace=True)\n",
    "    tmp_train_dat_TimeDiff[column] = encoder.transform(tmp_train_dat_TimeDiff[column])  \n",
    "    \n",
    "#'''\n",
    "\n",
    "\n",
    "## section to create timeStep features\n",
    "unique_value_counts = 10\n",
    "timeStep_columnNames = []\n",
    "column_name_columns = []\n",
    "for i in xrange(unique_value_counts):\n",
    "    timeStep_columnNames.extend(['time_diff_step_{}'.format(i), 'column_counts_step_{}'.format(i), \n",
    "                                 'time_cost_step_{}'.format(i), 'first_column_step_{}'.format(i)])\n",
    "    column_name_columns.append('first_column_step_{}'.format(i))\n",
    "\n",
    "tmp_train_dat_TimeStep = tmp_train_dat_Norm.apply(getTimeSteps, axis=1)\n",
    "tmp_train_dat_TimeStep.columns = timeStep_columnNames\n",
    "for column in column_name_columns:\n",
    "    tmp_train_dat_TimeStep[column].fillna('NaN', inplace=True)\n",
    "    tmp_train_dat_TimeStep[column] = encoder.transform(tmp_train_dat_TimeStep[column])\n",
    "    \n",
    "\n",
    "print 'finish feature engineering date using {} minutes'.format(round((time.time() - start_time)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_diff_start_col</th>\n",
       "      <th>time_diff_end_col</th>\n",
       "      <th>time_diff_value</th>\n",
       "      <th>time_ratio_value</th>\n",
       "      <th>first_time_value</th>\n",
       "      <th>last_time_value</th>\n",
       "      <th>first_date_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>1042</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.002003</td>\n",
       "      <td>1313.12</td>\n",
       "      <td>1315.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26</td>\n",
       "      <td>1042</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.000848</td>\n",
       "      <td>1662.63</td>\n",
       "      <td>1664.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>294</td>\n",
       "      <td>1042</td>\n",
       "      <td>13.14</td>\n",
       "      <td>1.016607</td>\n",
       "      <td>791.22</td>\n",
       "      <td>804.36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26</td>\n",
       "      <td>1042</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.010367</td>\n",
       "      <td>156.27</td>\n",
       "      <td>157.89</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>26</td>\n",
       "      <td>1042</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.001071</td>\n",
       "      <td>476.06</td>\n",
       "      <td>476.57</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_diff_start_col  time_diff_end_col  time_diff_value  time_ratio_value  \\\n",
       "Id                                                                              \n",
       "6                    26               1042             2.63          1.002003   \n",
       "14                   26               1042             1.41          1.000848   \n",
       "16                  294               1042            13.14          1.016607   \n",
       "23                   26               1042             1.62          1.010367   \n",
       "41                   26               1042             0.51          1.001071   \n",
       "\n",
       "    first_time_value  last_time_value  first_date_value  \n",
       "Id                                                       \n",
       "6            1313.12          1315.75               NaN  \n",
       "14           1662.63          1664.04               NaN  \n",
       "16            791.22           804.36               NaN  \n",
       "23            156.27           157.89               NaN  \n",
       "41            476.06           476.57               NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train_dat_TimeDiff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint tmp_train_dat_Norm.shape\\ntmp_train_dat_Norm.head()\\n\\nprint tmp_train_dat_TimeStep.shape\\ntmp_train_dat_TimeStep.head()\\n\\nprint tmp_train_dat_TimeDiff.shape\\ntmp_train_dat_TimeDiff.head()\\n\\nprint tmp_train_dat_Basics.shape\\ntmp_train_dat_Basics.head()\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print tmp_train_dat_Norm.shape\n",
    "tmp_train_dat_Norm.head()\n",
    "\n",
    "print tmp_train_dat_TimeStep.shape\n",
    "tmp_train_dat_TimeStep.head()\n",
    "\n",
    "print tmp_train_dat_TimeDiff.shape\n",
    "tmp_train_dat_TimeDiff.head()\n",
    "\n",
    "print tmp_train_dat_Basics.shape\n",
    "tmp_train_dat_Basics.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_train_dat = pd.concat([tmp_train_dat_Norm, tmp_train_dat_Basics, tmp_train_dat_TimeDiff, tmp_train_dat_TimeStep], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print combined_train_dat.max().max(), combined_train_dat.min().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill up NaN in the date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combined_train_dat.fillna(nan_fill_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print combined_train_dat.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#'start_time' in combined_train_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create features from index columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_train_dat['first_time_index']         = combined_train_dat['first_time_value'].argsort()\n",
    "combined_train_dat['last_time_index']          = combined_train_dat['last_time_value'].argsort()\n",
    "combined_train_dat['index_ratio']              = combined_train_dat['first_time_index'] / combined_train_dat['last_time_index']\n",
    "\n",
    "if 'start_time' in combined_train_dat.columns:\n",
    "    combined_train_dat['start_time_index']         = combined_train_dat['start_time'].argsort()\n",
    "    combined_train_dat['start_time_index_ratio_1'] = combined_train_dat['first_time_index'] / combined_train_dat['start_time_index']\n",
    "    combined_train_dat['start_time_index_ratio_2'] = combined_train_dat['last_time_index'] / combined_train_dat['start_time_index']\n",
    "    \n",
    "    \n",
    "combined_train_dat['time_ratio_value_index']    = combined_train_dat['time_ratio_value'].argsort()\n",
    "combined_train_dat['first_time_value_index']    = combined_train_dat['first_time_value'].argsort()\n",
    "combined_train_dat['first_date_value_index']    = combined_train_dat['first_date_value'].argsort()\n",
    "combined_train_dat['first_date_value_index_ratio_1'] = combined_train_dat['first_time_index'] / combined_train_dat['first_date_value_index']\n",
    "combined_train_dat['first_date_value_index_ratio_2'] = combined_train_dat['last_time_index'] / combined_train_dat['first_date_value_index']\n",
    "\n",
    "#combined_train_dat['index']                    = combined_train_cat.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nto_index_column_names = ['L0_S0_F20', 'L3_S33_F3859', 'L0_S1_F28', 'L3_S30_F3754', 'L3_S33_F3857',\\n                         'L0_S1_F24', 'L0_S2_F44', 'L3_S30_F3754', 'L3_S30_F3744']\\nfor col in to_index_column_names:\\n    col_name = '{}_index'.format(col)\\n    combined_train_num[col_name] = combined_train_num[col].argsort()\\n    if col != to_index_column_names[0]:\\n        ratio_col_name = '{}_index_ratio'.format(col)\\n        bench_col_name = col_name = '{}_index'.format(to_index_column_names[0])\\n        combined_train_num[ratio_col_name] = combined_train_num[col] / combined_train_num[bench_col_name]\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_num['top_2_ratio'] = combined_train_num['L3_S33_F3859'] / combined_train_num['L3_S33_F3857']\n",
    "\n",
    "'''\n",
    "to_index_column_names = ['L0_S0_F20', 'L3_S33_F3859', 'L0_S1_F28', 'L3_S30_F3754', 'L3_S33_F3857',\n",
    "                         'L0_S1_F24', 'L0_S2_F44', 'L3_S30_F3754', 'L3_S30_F3744']\n",
    "for col in to_index_column_names:\n",
    "    col_name = '{}_index'.format(col)\n",
    "    combined_train_num[col_name] = combined_train_num[col].argsort()\n",
    "    if col != to_index_column_names[0]:\n",
    "        ratio_col_name = '{}_index_ratio'.format(col)\n",
    "        bench_col_name = col_name = '{}_index'.format(to_index_column_names[0])\n",
    "        combined_train_num[ratio_col_name] = combined_train_num[col] / combined_train_num[bench_col_name]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train data dimension:  (100000, 967)\n",
      "processed train data dimension:  (100000, 967)\n",
      "raw train data dimension:  (100000, 1206)\n",
      "processed train data dimension:  (100000, 475)\n"
     ]
    }
   ],
   "source": [
    "#remove_single_value_columns(combined_train_cat)\n",
    "remove_single_value_columns(combined_train_num)\n",
    "remove_single_value_columns(combined_train_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### combine all the data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_train = pd.concat([combined_train_num, combined_train_dat, combined_train_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'start_time' in combined_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2975)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0_S1_F24</th>\n",
       "      <th>L0_S1_F28</th>\n",
       "      <th>L0_S2_F32</th>\n",
       "      <th>L0_S2_F36</th>\n",
       "      <th>L0_S2_F40</th>\n",
       "      <th>L0_S2_F44</th>\n",
       "      <th>L0_S2_F48</th>\n",
       "      <th>L0_S2_F52</th>\n",
       "      <th>L0_S2_F56</th>\n",
       "      <th>L0_S2_F60</th>\n",
       "      <th>...</th>\n",
       "      <th>level_16777216.0_column_counts</th>\n",
       "      <th>level_16777232.0_start_column</th>\n",
       "      <th>level_16777232.0_end_column</th>\n",
       "      <th>level_16777232.0_column_counts</th>\n",
       "      <th>level_16777557.0_start_column</th>\n",
       "      <th>level_16777557.0_end_column</th>\n",
       "      <th>level_16777557.0_column_counts</th>\n",
       "      <th>level_33554432.0_start_column</th>\n",
       "      <th>level_33554432.0_end_column</th>\n",
       "      <th>level_33554432.0_column_counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2975 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L0_S1_F24  L0_S1_F28  L0_S2_F32  L0_S2_F36  L0_S2_F40  L0_S2_F44  \\\n",
       "Id                                                                     \n",
       "6         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "14        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "16        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "23        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "41        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "    L0_S2_F48  L0_S2_F52  L0_S2_F56  L0_S2_F60  \\\n",
       "Id                                               \n",
       "6         NaN        NaN        NaN        NaN   \n",
       "14        NaN        NaN        NaN        NaN   \n",
       "16        NaN        NaN        NaN        NaN   \n",
       "23        NaN        NaN        NaN        NaN   \n",
       "41        NaN        NaN        NaN        NaN   \n",
       "\n",
       "                 ...                level_16777216.0_column_counts  \\\n",
       "Id               ...                                                 \n",
       "6                ...                                           NaN   \n",
       "14               ...                                           NaN   \n",
       "16               ...                                           NaN   \n",
       "23               ...                                           NaN   \n",
       "41               ...                                           NaN   \n",
       "\n",
       "    level_16777232.0_start_column  level_16777232.0_end_column  \\\n",
       "Id                                                               \n",
       "6                            1333                         1333   \n",
       "14                           1333                         1333   \n",
       "16                           1333                         1333   \n",
       "23                           1333                         1333   \n",
       "41                           1333                         1333   \n",
       "\n",
       "    level_16777232.0_column_counts  level_16777557.0_start_column  \\\n",
       "Id                                                                  \n",
       "6                              NaN                           1333   \n",
       "14                             NaN                           1333   \n",
       "16                             NaN                           1333   \n",
       "23                             NaN                           1333   \n",
       "41                             NaN                           1333   \n",
       "\n",
       "    level_16777557.0_end_column  level_16777557.0_column_counts  \\\n",
       "Id                                                                \n",
       "6                          1333                             NaN   \n",
       "14                         1333                             NaN   \n",
       "16                         1333                             NaN   \n",
       "23                         1333                             NaN   \n",
       "41                         1333                             NaN   \n",
       "\n",
       "    level_33554432.0_start_column  level_33554432.0_end_column  \\\n",
       "Id                                                               \n",
       "6                            1333                         1333   \n",
       "14                           1333                         1333   \n",
       "16                           1333                         1333   \n",
       "23                           1333                         1333   \n",
       "41                           1333                         1333   \n",
       "\n",
       "    level_33554432.0_column_counts  \n",
       "Id                                  \n",
       "6                              NaN  \n",
       "14                             NaN  \n",
       "16                             NaN  \n",
       "23                             NaN  \n",
       "41                             NaN  \n",
       "\n",
       "[5 rows x 2975 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print combined_train.shape\n",
    "combined_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combined_train = combined_train_dat\n",
    "#combined_train[dep_var_name] = combined_train_num[dep_var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############## Section of regular validation #######################\n",
    "train_index, valid_index = create_validation_index(combined_train, 0.3, dep_var_name, True)\n",
    "valid_data = combined_train.ix[valid_index]\n",
    "tmp_train  = combined_train.ix[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tmp_train[dep_var_name].values\n",
    "X = tmp_train.drop(dep_var_name, axis=1)\n",
    "\n",
    "valid_y = valid_data[dep_var_name].values\n",
    "valid_X = valid_data.drop(dep_var_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tree-base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_params = {'random_state' : 9999, 'n_estimators' : 2000, 'max_depth' : 7, 'criterion' : 'gini', 'n_jobs' : -1}\n",
    "et_params = {'random_state' : 9999, 'n_estimators' : 200, 'max_depth' : 18, 'criterion' : 'gini', 'n_jobs' : -1}\n",
    "rf_clf = RandomForestClassifier(**rf_params)\n",
    "rf_clf = rf_clf.fit(X, y)\n",
    "\n",
    "et_clf = RandomForestClassifier(**et_params)\n",
    "et_clf = et_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish training LR model using 242.0 seconds\n"
     ]
    }
   ],
   "source": [
    "C = 0.1\n",
    "start_time = time.time()\n",
    "LR_clf = LogisticRegression(C = C, class_weight='balanced', n_jobs = -1, penalty='l2')\n",
    "LR_clf.fit(X, y)\n",
    "print 'finish training LR model using {} seconds'.format(round(time.time() - start_time, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight: 159.185354691\n",
      "a base_score 0.00624276796046 is used in the xgboost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      " train the xgboost without early stopping\n",
      "####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.710775\n",
      "[50]\ttrain-auc:0.817682\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"eta\"]                      = 0.0075\n",
    "params[\"subsample\"]                = 0.8\n",
    "params[\"colsample_bytree\"]         = 0.8\n",
    "params[\"num_round\"]                = 1001\n",
    "params[\"max_depth\"]                = 5\n",
    "params[\"gamma\"]                    = 0\n",
    "params[\"metrics\"]                  = 'auc'\n",
    "params['eval_metric']              = 'auc'\n",
    "params[\"seed\"]                     = 999\n",
    "params['verbose_eval']             = 50\n",
    "## whether to use weights\n",
    "params['use_base_score']           = True\n",
    "params['use_weights']              = True\n",
    "#params['use_scale_pos_weight']     = True\n",
    "params[\"val\"]                      = False\n",
    "\n",
    "model = xgboost_classifier(label_name = dep_var_name, params = params, model_file='bosch_xgb_model')\n",
    "model.fit(tmp_train, dep_var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the prediction step, dep_var_name is not provided....\n",
      "result from using constant fraction: \n",
      "mean of groud truth: 0.00620020667356\n",
      "threshold for preds: 0.0333485260902\n",
      "0.210156570941\n",
      "\n",
      " \n",
      "\n",
      "result from using flexsible threshold: (0.2876271064191786, 0.3474218547344208)\n"
     ]
    }
   ],
   "source": [
    "#pred = rf_clf.predict_proba(valid_X)[:, 1]\n",
    "#pred = et_clf.predict_proba(valid_X)[:, 1]\n",
    "#pred = LR_clf.predict_proba(valid_X)[:, 1]\n",
    "pred = model.predict(valid_X)\n",
    "\n",
    "print 'result from using constant fraction: \\n', score_MCC(valid_y, pred)\n",
    "print '\\n \\n'\n",
    "print 'result from using flexsible threshold:', CombinedModel.mcc_eval_func(valid_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
