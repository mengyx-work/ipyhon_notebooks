{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yaml\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/home/ymm/bosch/'\n",
    "\n",
    "train_num_file   = 'train_numeric.csv'\n",
    "train_cat_file   = 'train_categorical.csv'\n",
    "train_date_file  = 'train_date.csv'\n",
    "test_num_file    = 'test_numeric.csv'\n",
    "test_cat_file    = 'test_categorical.csv'\n",
    "test_date_file   = 'test_date.csv'\n",
    "\n",
    "sample_submission_file   = 'sample_submission.csv'\n",
    "pickle_column_names_file = data_path + 'cat_col_names.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This section loads a fraction of categorical data and save the columns\n",
    "names into a pickle file named by 'pickle_column_names_file'.\n",
    "So that the following categorical data loading can directly use explicitly types\n",
    "'''\n",
    "\n",
    "def create_categorical_column_name_pickle(train_cat_file, pickle_column_names_file):\n",
    "    \n",
    "    tmp_train_cat = pd.read_csv(data_path + train_cat_file, index_col='Id', nrows=1000)\n",
    "\n",
    "    #for col, dtype in zip(tmp_train_cat.columns, tmp_train_cat.dtypes):\n",
    "    #    print len(train_cat[col].unique()), dtype\n",
    "\n",
    "    ## save the column names to pickle file\n",
    "    col_names = tmp_train_cat.columns.tolist()\n",
    "    with open(pickle_column_names_file, 'wb') as pickle_file:\n",
    "        pickle.dump(col_names, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time_column_name = 'L0_S0_D1'\n",
    "id_column_name = 'Id'\n",
    "dep_var_name = 'Response'\n",
    "\n",
    "## number of bins to separate data by start_time\n",
    "bin_num = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading takes  57.462346077  seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "## load the labels and start_time column for train and test data\n",
    "train_labels = pd.read_csv(data_path + train_num_file, index_col='Id', usecols=['Id', dep_var_name])\n",
    "train_date_start_columm = pd.read_csv(data_path + train_date_file, index_col='Id', usecols=['Id', start_time_column_name])\n",
    "test_date_start_columm = pd.read_csv(data_path + test_date_file, index_col='Id', usecols=['Id', start_time_column_name])\n",
    "end_time = time.time()\n",
    "print 'data loading takes ', round((end_time - start_time), 1), ' seconds.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183747, 2)\n"
     ]
    }
   ],
   "source": [
    "## join the start_time with labels, then drop the NaN in start_time\n",
    "labeled_start_time = pd.merge(train_labels, train_date_start_columm, how='left', left_index=True, right_index=True)\n",
    "print labeled_start_time.shape\n",
    "## this labeled_start_time dataFrame doesn't contain the NaN\n",
    "## can be directly used for calculating the mquantiles\n",
    "labeled_start_time = labeled_start_time[~labeled_start_time[start_time_column_name].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "section to subset the data by start_time\n",
    "'''\n",
    "from scipy.stats.mstats import mquantiles\n",
    "\n",
    "prob_list = [1.*i/bin_num for i in range(1, bin_num)]\n",
    "quantile_values = mquantiles(labeled_start_time[start_time_column_name], prob=prob_list)\n",
    "\n",
    "bins = [labeled_start_time[start_time_column_name].min()]\n",
    "bins.extend(quantile_values)\n",
    "bins.append(labeled_start_time[start_time_column_name].max())\n",
    "bin_names = [str(i) for i in range(len(bins)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## cut the entire dataframe into different time_windows by start_time\n",
    "tmp_train = train_date_start_columm.copy()\n",
    "tmp_test = test_date_start_columm.copy()\n",
    "\n",
    "tmp_train['time_window_num'] = pd.cut(tmp_train[start_time_column_name], bins, labels=bin_names)\n",
    "tmp_test['time_window_num'] = pd.cut(tmp_test[start_time_column_name], bins, labels=bin_names)\n",
    "## create a row number column, start index is 1\n",
    "tmp_train['row_num'] = range(1, (tmp_train.shape[0] + 1))\n",
    "tmp_test['row_num'] = range(1, (tmp_test.shape[0] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0_S0_D1</th>\n",
       "      <th>time_window_num</th>\n",
       "      <th>row_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82.24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1618.70</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1149.20</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>602.64</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    L0_S0_D1 time_window_num  row_num\n",
       "Id                                   \n",
       "4      82.24               0        1\n",
       "6        NaN             NaN        2\n",
       "7    1618.70              14        3\n",
       "9    1149.20               9        4\n",
       "11    602.64               5        5"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0_S0_D1</th>\n",
       "      <th>time_window_num</th>\n",
       "      <th>row_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>255.45</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    L0_S0_D1 time_window_num  row_num\n",
       "Id                                   \n",
       "1        NaN             NaN        1\n",
       "2        NaN             NaN        2\n",
       "3        NaN             NaN        3\n",
       "5     255.45               2        4\n",
       "8        NaN             NaN        5"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## sample for create the skipped row number list for row selection\\nselect_window_num = '0'\\nnone_selected_window_num = bin_names\\nnone_selected_window_num.append(np.NaN)\\nnone_selected_window_num.remove(select_window_num)\\n\\nskipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\\nskipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\\n\\nprint bin_names, len(bins)\\n\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## sample for create the skipped row number list for row selection\n",
    "select_window_num = '0'\n",
    "none_selected_window_num = bin_names\n",
    "none_selected_window_num.append(np.NaN)\n",
    "none_selected_window_num.remove(select_window_num)\n",
    "\n",
    "skipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "skipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "\n",
    "print bin_names, len(bins)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_date_data(train_date, test_date, start_time_column_name):\n",
    "    print 'raw date data dimension: ', train_date.shape, test_date.shape\n",
    "    train_date['start_time'] = train_date[start_time_column_name]\n",
    "    test_date['start_time'] = test_date[start_time_column_name]\n",
    "    single_value_column_names = []\n",
    "\n",
    "    for column in train_date.columns:\n",
    "        if column != 'start_time':\n",
    "            train_date[column] = train_date[column] - train_date['start_time']\n",
    "            test_date[column] = test_date[column] - test_date['start_time']\n",
    "        if len(train_date[column].unique()) == 1:\n",
    "            single_value_column_names.append(column)\n",
    "            \n",
    "    ## drop single-valued columns        \n",
    "    train_date.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    test_date.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    print 'processed date data dimension: ', train_date.shape, test_date.shape\n",
    "    \n",
    "\n",
    "    \n",
    "def remove_single_value_categorical_columns(train, test):\n",
    "    print 'raw categorical data dimension: ', train.shape, test.shape\n",
    "    single_value_column_names = []\n",
    "    for col in train.columns:\n",
    "        if len(train[col].unique()) == 1:\n",
    "            single_value_column_names.append(col)\n",
    "    \n",
    "    train.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    test.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    print 'processed categorical data dimension: ', train.shape, test.shape\n",
    "\n",
    "\n",
    "\n",
    "def encode_categorical_data(train, test, fill_missing = False):\n",
    "    '''\n",
    "    encoding is an extemely slow process\n",
    "    So only use the training data to trian the encoder\n",
    "    '''\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    if fill_missing:\n",
    "        train = train.fillna(value='missing')\n",
    "        test = test.fillna(value='missing')\n",
    "    \n",
    "    ## idealy combine the train and test\n",
    "    #combined = pd.concat([train, test], axis=0)\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    for col, dtype in zip(train.columns, train.dtypes):\n",
    "        if dtype == 'object':\n",
    "            le.fit(pd.concat([train[col], test[col]], axis=0))\n",
    "            train[col] = le.transform(train[col])\n",
    "            test[col] = le.transform(test[col])\n",
    "                              \n",
    "        counter += 1\n",
    "        if counter % 20 == 0:\n",
    "            print '{} out of {} is process...'.format(str(counter), str(train.shape[1]))\n",
    "                              \n",
    "    end_time = time.time()\n",
    "    print 'encoding process takes ', round((end_time - start_time)), 'seconds'\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "\n",
    "def load_data_by_index(skipped_train_row_num, skipped_test_row_num, train_data_file, test_data_file):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## section to load column names for categorical data\n",
    "    if not os.path.isfile(pickle_column_names_file):\n",
    "        print 'create new column name pickle file ...'\n",
    "        create_categorical_column_name_pickle(train_cat_file, pickle_column_names_file)\n",
    "    \n",
    "    with open(pickle_column_names_file, 'rb') as pickle_file:\n",
    "        cat_column_names = pickle.load(pickle_file)\n",
    "    \n",
    "    column_types = [np.object] * len(cat_column_names)\n",
    "    column_types_dict = dict(zip(cat_column_names, column_types))\n",
    "    ######\n",
    "\n",
    "    train_date = pd.read_csv(data_path + train_date_file, index_col='Id', skiprows=skipped_train_row_num)\n",
    "    train_num = pd.read_csv(data_path + train_num_file, index_col='Id', skiprows=skipped_train_row_num)\n",
    "    train_cat = pd.read_csv(data_path + train_cat_file, index_col='Id', skiprows=skipped_train_row_num, dtype=column_types_dict)\n",
    "\n",
    "    test_date = pd.read_csv(data_path + test_date_file, index_col='Id', skiprows=skipped_test_row_num)\n",
    "    test_num = pd.read_csv(data_path + test_num_file, index_col='Id', skiprows=skipped_test_row_num)\n",
    "    test_cat = pd.read_csv(data_path + test_cat_file, index_col='Id', skiprows=skipped_test_row_num, dtype=column_types_dict)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print 'data loading takes ', round((end_time - start_time), 2), 'seconds'\n",
    "    \n",
    "    ## process the date data\n",
    "    process_date_data(train_date, test_date, start_time_column_name)\n",
    "    print 'finish processing date data ...'\n",
    "    \n",
    "    ## process categorical data\n",
    "    remove_single_value_categorical_columns(train_cat, test_cat)\n",
    "    encode_categorical_data(train_cat, test_cat)\n",
    "    print 'finish processing categorical data ...'\n",
    "\n",
    "    ## combine the data and save into csv files\n",
    "    combined_train = pd.concat([train_cat, train_num, train_date], axis=1)\n",
    "    combined_test = pd.concat([test_cat, test_num, test_date], axis=1)\n",
    "    \n",
    "    combined_train.to_csv(train_data_file)\n",
    "    combined_test.to_csv(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading takes  555.32 seconds\n",
      "raw date data dimension:  (509886, 1156) (509245, 1156)\n",
      "processed date data dimension:  (509886, 173) (509245, 173)\n",
      "finish processing date data ...\n",
      "raw categorical data dimension:  (509886, 2140) (509245, 2140)\n",
      "processed categorical data dimension:  (509886, 1792) (509245, 1792)\n",
      "20 out of 1792 is process...\n",
      "40 out of 1792 is process...\n",
      "60 out of 1792 is process...\n",
      "80 out of 1792 is process...\n",
      "100 out of 1792 is process...\n",
      "120 out of 1792 is process...\n",
      "140 out of 1792 is process...\n",
      "160 out of 1792 is process...\n",
      "180 out of 1792 is process...\n",
      "200 out of 1792 is process...\n",
      "220 out of 1792 is process...\n",
      "240 out of 1792 is process...\n",
      "260 out of 1792 is process...\n",
      "280 out of 1792 is process...\n",
      "300 out of 1792 is process...\n",
      "320 out of 1792 is process...\n",
      "340 out of 1792 is process...\n",
      "360 out of 1792 is process...\n",
      "380 out of 1792 is process...\n"
     ]
    }
   ],
   "source": [
    "## dict for the yaml file\n",
    "subset_data_dict = {}\n",
    "\n",
    "## for the NaN start_time rows\n",
    "print 'for bin: NaN', \n",
    "none_selected_window_num = bin_names[:]\n",
    "skipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "skipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "selected_bin_name = 'NaN'\n",
    "subset_data_dict[selected_bin_name] = {}\n",
    "subset_data_dict[selected_bin_name]['start_time'] = 'NaN'\n",
    "subset_data_dict[selected_bin_name]['end_time'] = 'NaN'\n",
    "#print len(skipped_test_row_num), len(skipped_train_row_num), skipped_test_row_num[:10], skipped_train_row_num[:10]\n",
    "\n",
    "train_data_file = 'processed_totBins_' + str(len(bin_names)+1) + '_bin_' + selected_bin_name + '_train.csv'\n",
    "test_data_file = 'processed_totBins_' + str(len(bin_names)+1) + '_bin_' + selected_bin_name + '_test.csv'\n",
    "load_data_by_index(skipped_train_row_num, skipped_test_row_num, train_data_file, test_data_file)\n",
    "subset_data_dict[selected_bin_name]['train_file'] = train_data_file\n",
    "subset_data_dict[selected_bin_name]['test_file'] = test_data_file\n",
    "\n",
    "for selected_bin_name, i in zip(bin_names, range(len(bin_names))):\n",
    "    print 'for bin:', selected_bin_name\n",
    "    subset_data_dict[selected_bin_name] = {}\n",
    "    none_selected_window_num = bin_names[:]\n",
    "    none_selected_window_num.append(np.NaN)\n",
    "    none_selected_window_num.remove(selected_bin_name)\n",
    "    subset_data_dict[selected_bin_name]['start_time'] = round(float(bins[i]), 2)\n",
    "    subset_data_dict[selected_bin_name]['end_time'] = round(float(bins[i+1]), 2)\n",
    "\n",
    "    skipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "    skipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "    print len(skipped_test_row_num), len(skipped_train_row_num), skipped_test_row_num[:10], skipped_train_row_num[:10]\n",
    "    \n",
    "    train_data_file = 'processed_totBins_' + str(len(bin_names)+1) + '_bin_' + selected_bin_name + '_train.csv'\n",
    "    test_data_file = 'processed_totBins_' + str(len(bin_names)+1) + '_bin_' + selected_bin_name + '_test.csv'\n",
    "    load_data_by_index(skipped_train_row_num, skipped_test_row_num, train_data_file, test_data_file)\n",
    "    subset_data_dict[selected_bin_name]['train_file'] = train_data_file\n",
    "    subset_data_dict[selected_bin_name]['test_file'] = test_data_file\n",
    "\n",
    "\n",
    "print subset_data_dict\n",
    "with open('subsest_data_dict.yml', 'w') as outfile:\n",
    "    yaml.dump(subset_data_dict, outfile, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R&D steps for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create the skipped row number list for row selection\n",
    "select_window_num = np.NaN\n",
    "none_selected_window_num = bin_names\n",
    "none_selected_window_num.append(np.NaN)\n",
    "none_selected_window_num.remove(select_window_num)\n",
    "\n",
    "skipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "skipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674503 673861 [4, 6, 7, 8, 11, 12, 13, 14, 15, 16] [1, 3, 4, 5, 6, 9, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "print len(skipped_test_row_num), len(skipped_train_row_num), skipped_test_row_num[:10], skipped_train_row_num[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_index = tmp[tmp['time_window_num']=='0'].index.tolist()\n",
    "print 'selected row number: ', tmp.shape[0] - len(skipped_row_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading takes  91.98 seconds\n",
      "raw date data dimension:  (44909, 1156) (44980, 1156)\n",
      "processed date data dimension:  (44909, 495) (44980, 495)\n",
      "finish processing date data ...\n",
      "raw categorical data dimension:  (44909, 2140) (44980, 2140)\n",
      "processed categorical data dimension:  (44909, 436) (44980, 436)\n",
      "20 out of 436 is process...\n",
      "40 out of 436 is process...\n",
      "60 out of 436 is process...\n",
      "80 out of 436 is process...\n",
      "100 out of 436 is process...\n",
      "120 out of 436 is process...\n",
      "140 out of 436 is process...\n",
      "160 out of 436 is process...\n",
      "180 out of 436 is process...\n",
      "200 out of 436 is process...\n",
      "220 out of 436 is process...\n",
      "240 out of 436 is process...\n",
      "260 out of 436 is process...\n",
      "280 out of 436 is process...\n",
      "300 out of 436 is process...\n",
      "320 out of 436 is process...\n",
      "340 out of 436 is process...\n",
      "360 out of 436 is process...\n",
      "380 out of 436 is process...\n",
      "400 out of 436 is process...\n",
      "420 out of 436 is process...\n",
      "encoding process takes  173.0 seconds\n",
      "finish processing categorical data ...\n"
     ]
    }
   ],
   "source": [
    "load_data_by_index(skipped_train_row_num, skipped_test_row_num, 'tmp_train.csv', 'tmp_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_date_data(train_date, test_date, start_time_column_name):\n",
    "    print 'raw date data dimension: ', train_date.shape, test_date.shape\n",
    "    train_date['start_time'] = train_date[start_time_column_name]\n",
    "    test_date['start_time'] = test_date[start_time_column_name]\n",
    "    single_value_column_names = []\n",
    "\n",
    "    for column in train_date.columns:\n",
    "        if column != 'start_time':\n",
    "            train_date[column] = train_date[column] - train_date['start_time']\n",
    "            test_date[column] = test_date[column] - test_date['start_time']\n",
    "        if len(train_date[column].unique()) == 1:\n",
    "            single_value_column_names.append(column)\n",
    "            \n",
    "    ## drop single-valued columns        \n",
    "    train_date.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    test_date.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    print 'processed date data dimension: ', train_date.shape, test_date.shape\n",
    "    \n",
    "\n",
    "    \n",
    "def remove_single_value_categorical_columns(train, test):\n",
    "    print 'raw categorical data dimension: ', train.shape, test.shape\n",
    "    single_value_column_names = []\n",
    "    for col in train.columns:\n",
    "        if len(train[col].unique()) == 1:\n",
    "            single_value_column_names.append(col)\n",
    "    \n",
    "    train.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    test.drop(single_value_column_names, axis=1, inplace=True)\n",
    "    print 'processed categorical data dimension: ', train.shape, test.shape\n",
    "\n",
    "\n",
    "\n",
    "def encode_categorical_data(train, test, fill_missing = False):\n",
    "    '''\n",
    "    encoding is an extemely slow process\n",
    "    So only use the training data to trian the encoder\n",
    "    '''\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    if fill_missing:\n",
    "        train = train.fillna(value='missing')\n",
    "        test = test.fillna(value='missing')\n",
    "    \n",
    "    ## idealy combine the train and test\n",
    "    #combined = pd.concat([train, test], axis=0)\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    for col, dtype in zip(train.columns, train.dtypes):\n",
    "        if dtype == 'object':\n",
    "            le.fit(pd.concat([train[col], test[col]], axis=0))\n",
    "            train[col] = le.transform(train[col])\n",
    "            test[col] = le.transform(test[col])\n",
    "                              \n",
    "        counter += 1\n",
    "        if counter % 20 == 0:\n",
    "            print '{} out of {} is process...'.format(str(counter), str(train.shape[1]))\n",
    "                              \n",
    "    end_time = time.time()\n",
    "    print 'encoding process takes ', round((end_time - start_time)), 'seconds'\n",
    "    \n",
    "    \n",
    "\n",
    "def load_data_by_index(skipped_train_row_num, skipped_test_row_num, train_data_file, test_data_file):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## section to load column names for categorical data\n",
    "    if not os.path.isfile(pickle_column_names_file):\n",
    "        print 'create new column name pickle file ...'\n",
    "        create_categorical_column_name_pickle(train_cat_file, pickle_column_names_file)\n",
    "    \n",
    "    with open(pickle_column_names_file, 'rb') as pickle_file:\n",
    "        cat_column_names = pickle.load(pickle_file)\n",
    "    \n",
    "    column_types = [np.object] * len(cat_column_names)\n",
    "    column_types_dict = dict(zip(cat_column_names, column_types))\n",
    "    ######\n",
    "\n",
    "    train_date = pd.read_csv(data_path + train_date_file, index_col='Id', skiprows=skipped_train_row_num)\n",
    "    train_num = pd.read_csv(data_path + train_num_file, index_col='Id', skiprows=skipped_train_row_num)\n",
    "    train_cat = pd.read_csv(data_path + train_cat_file, index_col='Id', skiprows=skipped_train_row_num, dtype=column_types_dict)\n",
    "\n",
    "    test_date = pd.read_csv(data_path + test_date_file, index_col='Id', skiprows=skipped_test_row_num)\n",
    "    test_num = pd.read_csv(data_path + test_num_file, index_col='Id', skiprows=skipped_test_row_num)\n",
    "    test_cat = pd.read_csv(data_path + test_cat_file, index_col='Id', skiprows=skipped_test_row_num, dtype=column_types_dict)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print 'data loading takes ', round((end_time - start_time), 2), 'seconds'\n",
    "    \n",
    "    ## process the date data\n",
    "    process_date_data(train_date, test_date, start_time_column_name)\n",
    "    print 'finish processing date data ...'\n",
    "    \n",
    "    ## process categorical data\n",
    "    remove_single_value_categorical_columns(train_cat, test_cat)\n",
    "    encode_categorical_data(train_cat, test_cat)\n",
    "    print 'finish processing categorical data ...'\n",
    "\n",
    "    ## combine the data and save into csv files\n",
    "    combined_train = pd.concat([train_cat, train_num, train_date], axis=1)\n",
    "    combined_test = pd.concat([test_cat, test_num, test_date], axis=1)\n",
    "    \n",
    "    combined_train.to_csv(train_data_file)\n",
    "    combined_test.to_csv(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_train = pd.read_csv('tmp_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_test = pd.read_csv('tmp_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44909, 1901) (44980, 1900)\n"
     ]
    }
   ],
   "source": [
    "print tmp_train.shape, tmp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>L0_S1_F25</th>\n",
       "      <th>L0_S1_F27</th>\n",
       "      <th>L0_S1_F29</th>\n",
       "      <th>L0_S1_F31</th>\n",
       "      <th>L0_S2_F33</th>\n",
       "      <th>L0_S2_F35</th>\n",
       "      <th>L0_S2_F37</th>\n",
       "      <th>L0_S2_F39</th>\n",
       "      <th>L0_S2_F41</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S37_D3942</th>\n",
       "      <th>L3_S37_D3943</th>\n",
       "      <th>L3_S37_D3945</th>\n",
       "      <th>L3_S37_D3947</th>\n",
       "      <th>L3_S37_D3949</th>\n",
       "      <th>L3_S37_D3951</th>\n",
       "      <th>L3_S38_D3953</th>\n",
       "      <th>L3_S38_D3957</th>\n",
       "      <th>L3_S38_D3961</th>\n",
       "      <th>start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1618.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1633.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.77</td>\n",
       "      <td>16.77</td>\n",
       "      <td>16.77</td>\n",
       "      <td>16.77</td>\n",
       "      <td>16.77</td>\n",
       "      <td>16.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1674.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1658.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1662.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  L0_S1_F25  L0_S1_F27  L0_S1_F29  L0_S1_F31  L0_S2_F33  L0_S2_F35  \\\n",
       "0    7          0          0          0          0          0          0   \n",
       "1   38          0          0          0          0          0          0   \n",
       "2   82          0          0          0          0          0          0   \n",
       "3  205          0          0          0          0          0          0   \n",
       "4  330          0          0          0          0          0          0   \n",
       "\n",
       "   L0_S2_F37  L0_S2_F39  L0_S2_F41     ...      L3_S37_D3942  L3_S37_D3943  \\\n",
       "0          0          0          0     ...              5.72          5.72   \n",
       "1          0          0          0     ...              2.57          2.57   \n",
       "2          0          0          0     ...             16.77         16.77   \n",
       "3          0          0          0     ...              1.88          1.88   \n",
       "4          0          0          0     ...              1.48          1.48   \n",
       "\n",
       "   L3_S37_D3945  L3_S37_D3947  L3_S37_D3949  L3_S37_D3951  L3_S38_D3953  \\\n",
       "0          5.72          5.72          5.72          5.72           NaN   \n",
       "1          2.57          2.57          2.57          2.57           NaN   \n",
       "2         16.77         16.77         16.77         16.77           NaN   \n",
       "3          1.88          1.88          1.88          1.88           NaN   \n",
       "4          1.48          1.48          1.48          1.48           NaN   \n",
       "\n",
       "   L3_S38_D3957  L3_S38_D3961  start_time  \n",
       "0           NaN           NaN     1618.70  \n",
       "1           NaN           NaN     1633.80  \n",
       "2           NaN           NaN     1674.90  \n",
       "3           NaN           NaN     1658.58  \n",
       "4           NaN           NaN     1662.79  \n",
       "\n",
       "[5 rows x 1901 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd4f4b59410>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEDCAYAAAArwUMAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB7zkU9yE7SdlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4ef9a9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_train.start_time.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'start_time' in train_date.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0_S1_D26</th>\n",
       "      <th>L0_S1_D30</th>\n",
       "      <th>L0_S2_D34</th>\n",
       "      <th>L0_S2_D38</th>\n",
       "      <th>L0_S2_D42</th>\n",
       "      <th>L0_S2_D46</th>\n",
       "      <th>L0_S2_D50</th>\n",
       "      <th>L0_S2_D54</th>\n",
       "      <th>L0_S2_D58</th>\n",
       "      <th>L0_S2_D62</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S37_D3942</th>\n",
       "      <th>L3_S37_D3943</th>\n",
       "      <th>L3_S37_D3945</th>\n",
       "      <th>L3_S37_D3947</th>\n",
       "      <th>L3_S37_D3949</th>\n",
       "      <th>L3_S37_D3951</th>\n",
       "      <th>L3_S38_D3953</th>\n",
       "      <th>L3_S38_D3957</th>\n",
       "      <th>L3_S38_D3961</th>\n",
       "      <th>start_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1699.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1653.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1643.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1625.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1641.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     L0_S1_D26  L0_S1_D30  L0_S2_D34  L0_S2_D38  L0_S2_D42  L0_S2_D46  \\\n",
       "Id                                                                      \n",
       "22         0.0        0.0       0.00       0.00       0.00       0.00   \n",
       "69         0.0        0.0       0.00       0.00       0.00       0.00   \n",
       "74         0.0        0.0       0.01       0.01       0.01       0.01   \n",
       "105        0.0        0.0       0.00       0.00       0.00       0.00   \n",
       "195        0.0        0.0        NaN        NaN        NaN        NaN   \n",
       "\n",
       "     L0_S2_D50  L0_S2_D54  L0_S2_D58  L0_S2_D62     ...      L3_S37_D3942  \\\n",
       "Id                                                  ...                     \n",
       "22        0.00       0.00       0.00       0.00     ...             11.26   \n",
       "69        0.00       0.00       0.00       0.00     ...              6.81   \n",
       "74        0.01       0.01       0.01       0.01     ...              2.95   \n",
       "105       0.00       0.00       0.00       0.00     ...              2.20   \n",
       "195        NaN        NaN        NaN        NaN     ...              0.41   \n",
       "\n",
       "     L3_S37_D3943  L3_S37_D3945  L3_S37_D3947  L3_S37_D3949  L3_S37_D3951  \\\n",
       "Id                                                                          \n",
       "22          11.26         11.26         11.26         11.26         11.26   \n",
       "69           6.81          6.81          6.81          6.81          6.81   \n",
       "74           2.95          2.95          2.95          2.95          2.95   \n",
       "105          2.20          2.20          2.20          2.20          2.20   \n",
       "195          0.41          0.41          0.41          0.41          0.41   \n",
       "\n",
       "     L3_S38_D3953  L3_S38_D3957  L3_S38_D3961  start_time  \n",
       "Id                                                         \n",
       "22            NaN           NaN           NaN     1699.10  \n",
       "69            NaN           NaN           NaN     1653.24  \n",
       "74            NaN           NaN           NaN     1643.28  \n",
       "105           NaN           NaN           NaN     1625.58  \n",
       "195           NaN           NaN           NaN     1641.87  \n",
       "\n",
       "[5 rows x 495 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_date.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
