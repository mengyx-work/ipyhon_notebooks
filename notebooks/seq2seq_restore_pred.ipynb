{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import sys, os, time, math\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "sys.path.append('/Users/matt.meng/dev/seq2seq_model')\n",
    "from data import DataGenerator, process_batch\n",
    "from utils import model_meta_file\n",
    "from data_preprocess import TOKEN_DICT, _GO, _EOS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pickle_file = 'small_processed_titles_data.pkl'\n",
    "#local_model_path = '/Users/matt.meng/local_tensorflow_content/seq2seq'\n",
    "#local_model_path = '/Users/matt.meng/local_tensorflow_content/seq2seq_64embed_128hid'\n",
    "pickle_file = 'processed_titles_data.pkl'\n",
    "#local_model_path = '/Users/matt.meng/local_tensorflow_content/large_32embed_256hid'\n",
    "local_model_path = '/Users/matt.meng/local_tensorflow_content/large_32embed_64hid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open(pickle_file_path, 'rb') as input_stream:\n",
    "#    data = pickle.load(input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_output_c_name = 'encoder_decoder_sequence/encoder/while/Exit_2:0'\n",
    "encoder_output_h_name = 'encoder_decoder_sequence/encoder/while/Exit_3:0'\n",
    "\n",
    "\n",
    "pickle_file_path = os.path.join(os.path.expanduser(\"~\"), pickle_file)\n",
    "with open(pickle_file_path, 'rb') as input_stream:\n",
    "    data = pickle.load(input_stream)\n",
    "titles = data['titles']\n",
    "reverse_token_dict = data['reverse_token_dict']\n",
    "title_urls = data['url']\n",
    "title_pageViews = data['pageViw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url', 'reverse_token_dict', 'pageViw', 'titles', 'token_dict']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning, more than one model meta file is found in /Users/matt.meng/local_tensorflow_content/large_32embed_64hid\n",
      "INFO:tensorflow:Restoring parameters from /Users/matt.meng/local_tensorflow_content/large_32embed_64hid/models-1650000\n",
      "finish processing 5000 articles using 8.30 seconds...\n",
      "finish processing 10000 articles using 16.57 seconds...\n",
      "finish processing 15000 articles using 25.58 seconds...\n",
      "finish processing 20000 articles using 34.59 seconds...\n",
      "finish processing 25000 articles using 43.91 seconds...\n",
      "finish processing 30000 articles using 53.76 seconds...\n",
      "finish processing 35000 articles using 63.92 seconds...\n",
      "finish processing 40000 articles using 73.52 seconds...\n",
      "finish processing 45000 articles using 83.56 seconds...\n",
      "finish processing 50000 articles using 94.37 seconds...\n",
      "finish processing 55000 articles using 105.54 seconds...\n",
      "finish processing 60000 articles using 117.03 seconds...\n",
      "finish processing 65000 articles using 128.47 seconds...\n",
      "finish processing 70000 articles using 139.69 seconds...\n",
      "finish processing 75000 articles using 150.85 seconds...\n",
      "finish processing 80000 articles using 162.01 seconds...\n",
      "finish processing 85000 articles using 173.60 seconds...\n",
      "finish processing 90000 articles using 185.81 seconds...\n",
      "finish processing 95000 articles using 198.13 seconds...\n",
      "finish processing 100000 articles using 210.50 seconds...\n",
      "finish processing 105000 articles using 222.56 seconds...\n",
      "finish processing 110000 articles using 234.61 seconds...\n",
      "finish processing 115000 articles using 246.59 seconds...\n",
      "finish processing 120000 articles using 258.61 seconds...\n",
      "finish processing 125000 articles using 270.61 seconds...\n",
      "finish processing 130000 articles using 283.06 seconds...\n",
      "finish processing 135000 articles using 296.28 seconds...\n",
      "finish processing 140000 articles using 309.59 seconds...\n",
      "finish processing 145000 articles using 322.80 seconds...\n",
      "finish processing 150000 articles using 335.98 seconds...\n",
      "finish processing 155000 articles using 349.19 seconds...\n",
      "finish processing 160000 articles using 362.28 seconds...\n",
      "finish processing 165000 articles using 375.37 seconds...\n",
      "finish processing 170000 articles using 388.43 seconds...\n",
      "finish processing 175000 articles using 401.55 seconds...\n",
      "finish processing 180000 articles using 414.60 seconds...\n",
      "finish processing 185000 articles using 428.53 seconds...\n",
      "finish processing 190000 articles using 445.01 seconds...\n",
      "finish processing 195000 articles using 459.35 seconds...\n",
      "finish processing 200000 articles using 474.78 seconds...\n",
      "finish processing 205000 articles using 491.33 seconds...\n",
      "finish processing 210000 articles using 508.49 seconds...\n",
      "finish processing 215000 articles using 524.83 seconds...\n",
      "finish processing 220000 articles using 540.31 seconds...\n",
      "finish processing 225000 articles using 555.67 seconds...\n",
      "finish processing 230000 articles using 569.93 seconds...\n",
      "finish processing 235000 articles using 584.10 seconds...\n",
      "finish processing 240000 articles using 601.18 seconds...\n",
      "finish processing 245000 articles using 620.89 seconds...\n",
      "finish processing 250000 articles using 638.76 seconds...\n",
      "finish processing 255000 articles using 655.56 seconds...\n",
      "finish processing 260000 articles using 671.49 seconds...\n",
      "finish processing 265000 articles using 687.58 seconds...\n",
      "finish processing 270000 articles using 703.93 seconds...\n",
      "finish processing 275000 articles using 719.41 seconds...\n",
      "finish processing 280000 articles using 734.81 seconds...\n",
      "finish processing 285000 articles using 750.87 seconds...\n",
      "finish processing 290000 articles using 767.65 seconds...\n",
      "finish processing 295000 articles using 786.48 seconds...\n",
      "finish processing 300000 articles using 803.56 seconds...\n",
      "finish processing 305000 articles using 820.77 seconds...\n",
      "finish processing 310000 articles using 838.09 seconds...\n",
      "finish processing 315000 articles using 855.00 seconds...\n",
      "finish processing 320000 articles using 871.24 seconds...\n",
      "finish processing 325000 articles using 887.90 seconds...\n",
      "finish processing 330000 articles using 903.60 seconds...\n",
      "finish processing 335000 articles using 919.70 seconds...\n",
      "finish processing 340000 articles using 937.52 seconds...\n",
      "finish processing 345000 articles using 955.11 seconds...\n",
      "finish processing 350000 articles using 972.36 seconds...\n",
      "finish processing 355000 articles using 990.64 seconds...\n",
      "finish processing 360000 articles using 1011.30 seconds...\n",
      "finish processing 365000 articles using 1032.30 seconds...\n",
      "finish processing 370000 articles using 1054.63 seconds...\n",
      "finish processing 375000 articles using 1073.24 seconds...\n",
      "finish processing 380000 articles using 1093.25 seconds...\n",
      "finish processing 385000 articles using 1113.77 seconds...\n",
      "finish processing 390000 articles using 1133.17 seconds...\n",
      "finish processing 395000 articles using 1152.39 seconds...\n"
     ]
    }
   ],
   "source": [
    "#c_vector_list, h_vector_list = [], []\n",
    "vector_dict = {}\n",
    "saver = tf.train.import_meta_graph(model_meta_file(local_model_path))\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(local_model_path))\n",
    "    \n",
    "    encoder_inputs = sess.graph.get_tensor_by_name(\"initial_inputs/encoder_inputs:0\")\n",
    "    decoder_inputs = sess.graph.get_tensor_by_name(\"initial_inputs/decoder_inputs:0\")\n",
    "    decoder_targets = sess.graph.get_tensor_by_name(\"initial_inputs/decoder_targets:0\")\n",
    "    dropout_input_keep_prob = sess.graph.get_tensor_by_name(\"initial_inputs/dropout_input_keep_prob:0\")\n",
    "\n",
    "    encoder_output_c = sess.graph.get_tensor_by_name('encoder_decoder_sequence/encoder/while/Exit_2:0')\n",
    "    encoder_output_h = sess.graph.get_tensor_by_name('encoder_decoder_sequence/encoder/while/Exit_3:0')\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    for i in xrange(len(titles)):\n",
    "        counter += 1\n",
    "        vector_dict[title_urls[i]] = {'pageView' : title_pageViews[i], \n",
    "                                      'title_string' : ' '.join(map(reverse_token_dict.get, titles[i]))}\n",
    "        #title = ' '.join(map(reverse_token_dict.get, titles[i]))\n",
    "        encoder_inputs_, _ = process_batch([titles[i] + [TOKEN_DICT[_EOS]]])\n",
    "        decoder_targets_, _ = process_batch([titles[i] + [TOKEN_DICT[_EOS]]])\n",
    "        decoder_inputs_, _ = process_batch([[TOKEN_DICT[_GO]] + titles[i]])\n",
    "        c_vector, h_vector = sess.run([encoder_output_c, encoder_output_h], {encoder_inputs : encoder_inputs_,\n",
    "                                                                             decoder_inputs : decoder_inputs_, \n",
    "                                                                             decoder_targets : decoder_targets_, \n",
    "                                                                             dropout_input_keep_prob : 1.})\n",
    "        vector_dict[title_urls[i]]['vectors'] = (c_vector[0] + h_vector[0])\n",
    "        #c_vector_list.append(c_vector)\n",
    "        #h_vector_list.append(h_vector)\n",
    "        if counter % 5000 == 0:\n",
    "            print 'finish processing {} articles using {:.2f} seconds...'.format(counter, time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the difference is caused by duplicate URLs\n",
    "print len(title_pageViews), len(vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = vector_dict.keys()[10]\n",
    "print vector_dict[key].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_items = vector_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_vecor = sorted(vector_items, key=lambda x: x[1]['pageView'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_vecor[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vector_dict['http://www.chicagotribune.com/lifestyles/sns-201706141112--tms--hscopebctnzz-a20170708-20170708-story.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_rooted(x):\n",
    "    return math.sqrt(sum([a*a for a in x]))\n",
    " \n",
    "def euclidean_distance(x,y):\n",
    "     return math.sqrt(sum(math.pow(a - b, 2) for a, b in zip(x, y)))\n",
    "\n",
    "def manhattan_distance(x,y): \n",
    "    return sum(abs(a - b) for a,b in zip(x,y))\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    numerator = sum(a*b for a, b in zip(x,y))\n",
    "    denominator = square_rooted(x) * square_rooted(y)\n",
    "    #denominator = (square_rooted(x) + square_rooted(y))\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 230\n",
    "print sorted_vecor[index][1]['title_string']\n",
    "selected_key = sorted_vecor[index][0]\n",
    "print selected_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use `index` to loop through all the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_results, euclidean_results = [], []\n",
    "#selected_key = 'http://www.cbssports.com/nba/news/nba-draft-lavar-ball-guarantees-lonzo-will-lead-lakers-to-playoffs-next-season/'\n",
    "print selected_key\n",
    "print '\\n \\n'\n",
    "start_time = time.time()\n",
    "#for i in xrange(len(sorted_vecor)):\n",
    "for i in xrange(100000):\n",
    "    if i == index:\n",
    "        continue\n",
    "    cosine_result = cosine_similarity(sorted_vecor[index][1]['vectors'], sorted_vecor[i][1]['vectors'])\n",
    "    euclidean_result = euclidean_distance(sorted_vecor[index][1]['vectors'], sorted_vecor[i][1]['vectors'])\n",
    "    cosine_results.append(cosine_result)\n",
    "    euclidean_results.append(euclidean_result)\n",
    "print 'all the process takes {:.2f} seconds...'.format(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.hist(euclidean_results, bins=50)\n",
    "plt.hist(cosine_results, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cosine_results, euclidean_results = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_threshold = 0.9\n",
    "euclidean_threshold = 15\n",
    "\n",
    "#'''\n",
    "for i in xrange(len(euclidean_results)):\n",
    "    if euclidean_results[i] < euclidean_threshold:\n",
    "        print sorted_vecor[i][0], sorted_vecor[i][1]['title_string']\n",
    "#'''\n",
    "\n",
    "'''        \n",
    "for i in xrange(len(cosine_results)):\n",
    "    if cosine_results[i] > cosine_threshold:\n",
    "        print cosine_results[i]\n",
    "        print sorted_vecor[i][0], sorted_vecor[i][1]['title_string']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the `key` to loop through all the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "#selected_key = 'http://www.cbssports.com/nba/news/nba-draft-lavar-ball-guarantees-lonzo-will-lead-lakers-to-playoffs-next-season/'\n",
    "print selected_key\n",
    "print '\\n \\n'\n",
    "for key in vector_dict.keys():\n",
    "    if key == selected_key:\n",
    "        continue\n",
    "    #vector_index = 0\n",
    "    #cosine_result = cosine_similarity(vector_dict[selected_key]['vectors'][vector_index], vector_dict[key]['vectors'][vector_index])\n",
    "    #euclidean_result = euclidean_distance(vector_dict[selected_key]['vectors'][vector_index], vector_dict[key]['vectors'][vector_index])\n",
    "    #result = cosine_result\n",
    "    start_time = time.time()\n",
    "    cosine_result = cosine_similarity(vector_dict[selected_key]['vectors'], vector_dict[key]['vectors'])\n",
    "    euclidean_result = euclidean_distance(vector_dict[selected_key]['vectors'], vector_dict[key]['vectors'])\n",
    "    result = cosine_result\n",
    "    print 'all the process takes {:.2f} seconds...'.format(time.time() - start_time)\n",
    "    #result = euclidean_result\n",
    "    #result = 10 * (1 - cosine_result) * euclidean_result\n",
    "    #result = manhattan_distance(vector_dict[selected_key]['vectors'][0], vector_dict[key]['vectors'][0])\n",
    "    #if result < 18:\n",
    "    #if result > 0.92:\n",
    "        #print key, result\n",
    "        #print vector_dict[key]['pageView']\n",
    "    \n",
    "    #print cosine_result, euclidean_result, result\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(results, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
