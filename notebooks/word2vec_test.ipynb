{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_window = 2\n",
    "num_skips = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "buffer = collections.deque(maxlen=span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer.extend(xrange(span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [skip_window]\n",
    "    for j in range(num_skips):\n",
    "        while target in targets_to_avoid:\n",
    "            target = random.randint(0, span - 1)\n",
    "        targets_to_avoid.append(target)\n",
    "        batch[i * num_skips + j] = buffer[skip_window]\n",
    "        labels[i * num_skips + j, 0] = buffer[target]\n",
    "   \n",
    "    buffer.append(data_index)\n",
    "    data_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.randint(1000, size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, math, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class word2vec(object):\n",
    "    \n",
    "    def __init__(self, config, model_type='CBOW'):\n",
    "        self.model_type = model_type\n",
    "        self.config = config\n",
    "        \n",
    "        #assert config.batch_size % config.num_skip == 0\n",
    "        #assert config.num_skip <= 2 * config.context_window\n",
    "        \n",
    "    def _init_placeholders(self):\n",
    "        if self.model_type == 'CBOW':\n",
    "            self.X = tf.placeholder(tf.int32, shape=[self.config.batch_size, self.config.context_window*2], name=\"input_X\")\n",
    "        elif self.model_type == 'SKIP_GRAM':\n",
    "            self.X = tf.placeholder(tf.int32, shape=[self.config.batch_size], name=\"input_X\")\n",
    "        else:\n",
    "            raise ValueError('unknown model type {} is found...'.format(self.model_type))\n",
    "        self.y = tf.placeholder(tf.int32, shape=[self.config.batch_size, 1], name=\"input_y\")\n",
    "        \n",
    "    def _init_variables(self):\n",
    "        init_width = 0.5 / self.config.embedding_size\n",
    "        self.embedding = tf.Variable(\n",
    "            tf.random_uniform([self.config.vocab_size, self.config.embedding_size], -init_width, init_width),\n",
    "            name='embedding')\n",
    "        self.weight = tf.Variable(\n",
    "            tf.truncated_normal([self.config.vocab_size, self.config.embedding_size], stddev=1. / math.sqrt(self.config.embedding_size)),\n",
    "            name='weight')\n",
    "        self.bias = tf.Variable(tf.zeros([self.config.vocab_size]), name='bias')\n",
    "        \n",
    "    def cbow_batch_content(self):\n",
    "        span = 2 * self.config.context_window + 1\n",
    "        X = np.zeros(shape=(self.config.batch_size, span-1), dtype=np.int32)\n",
    "        y = np.zeros(shape=(self.config.batch_size, 1), dtype=np.int32)\n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        buffer.extend(np.random.randint(self.config.vocab_size, size=span))\n",
    "        for i in xrange(self.config.batch_size):\n",
    "            buffer_list = list(buffer)\n",
    "            y[i, 0] = buffer_list.pop(self.config.context_window)\n",
    "            X[i] = buffer_list\n",
    "            buffer.append(np.random.randint(self.config.vocab_size, size=1))\n",
    "        return X, y\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        X_embedded = tf.nn.embedding_lookup(self.embedding, self.X)\n",
    "        if self.model_type == 'CBOW':\n",
    "            X_embedded = tf.reduce_sum(X_embedded, 1)\n",
    "        print 'shape: ', X_embedded\n",
    "        self.loss = tf.reduce_mean(tf.nn.nce_loss(self.weight, \n",
    "                                             self.bias, \n",
    "                                             inputs=X_embedded, \n",
    "                                             labels=self.y, \n",
    "                                             num_sampled=self.config.neg_sample_size,\n",
    "                                             num_classes=self.config.vocab_size))\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def train(self):\n",
    "        self._init_placeholders()\n",
    "        self._init_variables()\n",
    "        self._build_graph()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            step = 0\n",
    "            while step < 50000:\n",
    "                step += 1\n",
    "                X, y = self.cbow_batch_content()\n",
    "                _, loss = sess.run([self.train, self.loss], feed_dict={self.X: X, \n",
    "                                                                       self.y: y})\n",
    "                print 'the loss: {} at step {}'.format(loss, step)\n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    class ModelConfig():\n",
    "        pass\n",
    "    model_config = ModelConfig()\n",
    "    model_config.batch_size = 32\n",
    "    model_config.context_window = 2\n",
    "    model_config.vocab_size = 2000\n",
    "    model_config.embedding_size = 512\n",
    "    model_config.neg_sample_size = 2\n",
    "    model_config.learning_rate = 0.0001\n",
    "    model = word2vec(model_config)\n",
    "    model.train()\n",
    "    #X_, y_ = model.cbow_batch_content()\n",
    "    #print X_\n",
    "    #print y_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  Tensor(\"Sum_6:0\", shape=(32, 512), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/matt.meng/.virtualenvs/kaggle/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "the loss: 16.3976764679 at step 1\n",
      "the loss: 16.2923965454 at step 2\n",
      "the loss: 5.6710486412 at step 3\n",
      "the loss: 14.4978618622 at step 4\n",
      "the loss: 13.0691719055 at step 5\n",
      "the loss: 13.4176864624 at step 6\n",
      "the loss: 10.8247394562 at step 7\n",
      "the loss: 10.1176319122 at step 8\n",
      "the loss: 9.24762153625 at step 9\n",
      "the loss: 6.77455329895 at step 10\n",
      "the loss: 11.7266397476 at step 11\n",
      "the loss: 5.11419296265 at step 12\n",
      "the loss: 9.13751125336 at step 13\n",
      "the loss: 11.7613449097 at step 14\n",
      "the loss: 11.8041944504 at step 15\n",
      "the loss: 6.88624000549 at step 16\n",
      "the loss: 7.78710317612 at step 17\n",
      "the loss: 9.7070941925 at step 18\n",
      "the loss: 4.62327003479 at step 19\n",
      "the loss: 12.3694953918 at step 20\n",
      "the loss: 5.74902439117 at step 21\n",
      "the loss: 5.90933036804 at step 22\n",
      "the loss: 14.9038801193 at step 23\n",
      "the loss: 5.89936447144 at step 24\n",
      "the loss: 10.1395282745 at step 25\n",
      "the loss: 15.6779975891 at step 26\n",
      "the loss: 9.62950325012 at step 27\n",
      "the loss: 9.5205745697 at step 28\n",
      "the loss: 4.20828294754 at step 29\n",
      "the loss: 4.75984954834 at step 30\n",
      "the loss: 10.3091182709 at step 31\n",
      "the loss: 9.44258117676 at step 32\n",
      "the loss: 7.92697906494 at step 33\n",
      "the loss: 8.92742919922 at step 34\n",
      "the loss: 12.1837177277 at step 35\n",
      "the loss: 10.3172721863 at step 36\n",
      "the loss: 13.0629358292 at step 37\n",
      "the loss: 10.6984004974 at step 38\n",
      "the loss: 8.36731910706 at step 39\n",
      "the loss: 8.4643907547 at step 40\n",
      "the loss: 11.0004081726 at step 41\n",
      "the loss: 4.7486333847 at step 42\n",
      "the loss: 9.87393188477 at step 43\n",
      "the loss: 5.89418697357 at step 44\n",
      "the loss: 10.6505632401 at step 45\n",
      "the loss: 14.9680233002 at step 46\n",
      "the loss: 10.0338792801 at step 47\n",
      "the loss: 13.0483112335 at step 48\n",
      "the loss: 9.88642406464 at step 49\n",
      "the loss: 12.2802791595 at step 50\n",
      "the loss: 9.89120292664 at step 51\n",
      "the loss: 10.687286377 at step 52\n",
      "the loss: 13.5439891815 at step 53\n",
      "the loss: 11.1515254974 at step 54\n",
      "the loss: 7.64959049225 at step 55\n",
      "the loss: 12.0438699722 at step 56\n",
      "the loss: 15.6832857132 at step 57\n",
      "the loss: 9.52133369446 at step 58\n",
      "the loss: 3.9197883606 at step 59\n",
      "the loss: 6.27073574066 at step 60\n",
      "the loss: 13.4355106354 at step 61\n",
      "the loss: 11.9771022797 at step 62\n",
      "the loss: 7.91584062576 at step 63\n",
      "the loss: 10.1508388519 at step 64\n",
      "the loss: 11.9663333893 at step 65\n",
      "the loss: 7.97369241714 at step 66\n",
      "the loss: 11.4879875183 at step 67\n",
      "the loss: 14.0823945999 at step 68\n",
      "the loss: 11.4542293549 at step 69\n",
      "the loss: 10.1716594696 at step 70\n",
      "the loss: 15.1940221786 at step 71\n",
      "the loss: 9.9967918396 at step 72\n",
      "the loss: 10.2755527496 at step 73\n",
      "the loss: 7.93933582306 at step 74\n",
      "the loss: 12.833108902 at step 75\n",
      "the loss: 13.5572605133 at step 76\n",
      "the loss: 9.76327610016 at step 77\n",
      "the loss: 10.0883712769 at step 78\n",
      "the loss: 12.6833562851 at step 79\n",
      "the loss: 5.84168291092 at step 80\n",
      "the loss: 12.1135845184 at step 81\n",
      "the loss: 4.75130271912 at step 82\n",
      "the loss: 4.9895324707 at step 83\n",
      "the loss: 10.336894989 at step 84\n",
      "the loss: 8.8384103775 at step 85\n",
      "the loss: 14.5774269104 at step 86\n",
      "the loss: 16.0686454773 at step 87\n",
      "the loss: 11.5511054993 at step 88\n",
      "the loss: 15.1513843536 at step 89\n",
      "the loss: 13.087594986 at step 90\n",
      "the loss: 16.6071434021 at step 91\n",
      "the loss: 10.056350708 at step 92\n",
      "the loss: 10.3361148834 at step 93\n",
      "the loss: 9.23417282104 at step 94\n",
      "the loss: 11.1810112 at step 95\n",
      "the loss: 14.128818512 at step 96\n",
      "the loss: 12.6107988358 at step 97\n",
      "the loss: 15.0951251984 at step 98\n",
      "the loss: 5.81645393372 at step 99\n",
      "the loss: 7.70253562927 at step 100\n",
      "the loss: 7.40723133087 at step 101\n",
      "the loss: 9.53014373779 at step 102\n",
      "the loss: 6.98383903503 at step 103\n",
      "the loss: 9.66806411743 at step 104\n",
      "the loss: 11.8109045029 at step 105\n",
      "the loss: 12.5004730225 at step 106\n",
      "the loss: 14.3038167953 at step 107\n",
      "the loss: 13.0118494034 at step 108\n",
      "the loss: 8.39254951477 at step 109\n",
      "the loss: 9.78352355957 at step 110\n",
      "the loss: 16.5665588379 at step 111\n",
      "the loss: 9.64382076263 at step 112\n",
      "the loss: 14.3439760208 at step 113\n",
      "the loss: 10.8282470703 at step 114\n",
      "the loss: 11.0358219147 at step 115\n",
      "the loss: 11.2381820679 at step 116\n",
      "the loss: 15.3772525787 at step 117\n",
      "the loss: 9.41441345215 at step 118\n",
      "the loss: 10.2965316772 at step 119\n",
      "the loss: 10.1822853088 at step 120\n",
      "the loss: 11.4574623108 at step 121\n",
      "the loss: 10.4855175018 at step 122\n",
      "the loss: 12.7975912094 at step 123\n",
      "the loss: 13.0995988846 at step 124\n",
      "the loss: 8.01166915894 at step 125\n",
      "the loss: 6.17351436615 at step 126\n",
      "the loss: 15.664056778 at step 127\n",
      "the loss: 6.15944766998 at step 128\n",
      "the loss: 12.5678195953 at step 129\n",
      "the loss: 12.2164459229 at step 130\n",
      "the loss: 6.28787136078 at step 131\n",
      "the loss: 11.9225196838 at step 132\n",
      "the loss: 7.19235229492 at step 133\n",
      "the loss: 9.17235183716 at step 134\n",
      "the loss: 5.71228122711 at step 135\n",
      "the loss: 9.20112228394 at step 136\n",
      "the loss: 11.519405365 at step 137\n",
      "the loss: 12.9347267151 at step 138\n",
      "the loss: 11.8455791473 at step 139\n",
      "the loss: 9.02874565125 at step 140\n",
      "the loss: 8.28517055511 at step 141\n",
      "the loss: 13.7803916931 at step 142\n",
      "the loss: 6.0245885849 at step 143\n",
      "the loss: 13.2877578735 at step 144\n",
      "the loss: 6.26370096207 at step 145\n",
      "the loss: 10.8041191101 at step 146\n",
      "the loss: 14.0419502258 at step 147\n",
      "the loss: 5.70754241943 at step 148\n",
      "the loss: 11.7112312317 at step 149\n",
      "the loss: 6.42580413818 at step 150\n",
      "the loss: 11.8534622192 at step 151\n",
      "the loss: 9.90531253815 at step 152\n",
      "the loss: 7.95992088318 at step 153\n",
      "the loss: 9.48377799988 at step 154\n",
      "the loss: 11.2599315643 at step 155\n",
      "the loss: 8.45792961121 at step 156\n",
      "the loss: 6.41910076141 at step 157\n",
      "the loss: 13.6402196884 at step 158\n",
      "the loss: 11.3286876678 at step 159\n",
      "the loss: 8.29732704163 at step 160\n",
      "the loss: 8.32716941833 at step 161\n",
      "the loss: 7.37321472168 at step 162\n",
      "the loss: 9.52788352966 at step 163\n",
      "the loss: 9.08363628387 at step 164\n",
      "the loss: 9.88224601746 at step 165\n",
      "the loss: 7.14772510529 at step 166\n",
      "the loss: 11.6271085739 at step 167\n",
      "the loss: 6.61296796799 at step 168\n",
      "the loss: 13.7144813538 at step 169\n",
      "the loss: 9.80576992035 at step 170\n",
      "the loss: 6.24914073944 at step 171\n",
      "the loss: 10.6233730316 at step 172\n",
      "the loss: 14.0909557343 at step 173\n",
      "the loss: 12.745593071 at step 174\n",
      "the loss: 6.31618404388 at step 175\n",
      "the loss: 13.1989383698 at step 176\n",
      "the loss: 11.3158321381 at step 177\n",
      "the loss: 10.1530923843 at step 178\n",
      "the loss: 5.64755725861 at step 179\n",
      "the loss: 10.9627437592 at step 180\n",
      "the loss: 5.21795225143 at step 181\n",
      "the loss: 9.59206199646 at step 182\n",
      "the loss: 15.6099185944 at step 183\n",
      "the loss: 10.2299137115 at step 184\n",
      "the loss: 8.52693939209 at step 185\n",
      "the loss: 10.8057031631 at step 186\n",
      "the loss: 5.44850254059 at step 187\n",
      "the loss: 5.09065294266 at step 188\n",
      "the loss: 10.1409358978 at step 189\n",
      "the loss: 11.8302669525 at step 190\n",
      "the loss: 8.61518669128 at step 191\n",
      "the loss: 6.2459602356 at step 192\n",
      "the loss: 9.52653503418 at step 193\n",
      "the loss: 4.16873168945 at step 194\n",
      "the loss: 6.44002532959 at step 195\n",
      "the loss: 11.9760789871 at step 196\n",
      "the loss: 9.74694442749 at step 197\n",
      "the loss: 6.11774682999 at step 198\n",
      "the loss: 11.0908517838 at step 199\n",
      "the loss: 9.54642486572 at step 200\n",
      "the loss: 7.23731422424 at step 201\n",
      "the loss: 9.39715194702 at step 202\n",
      "the loss: 4.48952770233 at step 203\n",
      "the loss: 10.1165046692 at step 204\n",
      "the loss: 5.94357728958 at step 205\n",
      "the loss: 13.6568460464 at step 206\n",
      "the loss: 6.90178585052 at step 207\n",
      "the loss: 8.20259094238 at step 208\n",
      "the loss: 11.4990062714 at step 209\n",
      "the loss: 8.96041297913 at step 210\n",
      "the loss: 14.1535606384 at step 211\n",
      "the loss: 4.96895456314 at step 212\n",
      "the loss: 10.9616146088 at step 213\n",
      "the loss: 9.8926448822 at step 214\n",
      "the loss: 14.9442033768 at step 215\n",
      "the loss: 12.7164363861 at step 216\n",
      "the loss: 10.141500473 at step 217\n",
      "the loss: 7.56760787964 at step 218\n",
      "the loss: 14.2805662155 at step 219\n",
      "the loss: 7.28383731842 at step 220\n",
      "the loss: 13.7654914856 at step 221\n",
      "the loss: 6.2679977417 at step 222\n",
      "the loss: 8.63830947876 at step 223\n",
      "the loss: 12.8182868958 at step 224\n",
      "the loss: 5.93367528915 at step 225\n",
      "the loss: 7.15509414673 at step 226\n",
      "the loss: 8.5716381073 at step 227\n",
      "the loss: 9.59538650513 at step 228\n",
      "the loss: 11.4870986938 at step 229\n",
      "the loss: 6.68723726273 at step 230\n",
      "the loss: 13.983165741 at step 231\n",
      "the loss: 14.0814590454 at step 232\n",
      "the loss: 12.8700714111 at step 233\n",
      "the loss: 7.79026556015 at step 234\n",
      "the loss: 11.3761901855 at step 235\n",
      "the loss: 13.5203952789 at step 236\n",
      "the loss: 12.6690750122 at step 237\n",
      "the loss: 8.73169994354 at step 238\n",
      "the loss: 14.2604103088 at step 239\n",
      "the loss: 9.79156780243 at step 240\n",
      "the loss: 7.10475254059 at step 241\n",
      "the loss: 10.3102855682 at step 242\n",
      "the loss: 13.2910938263 at step 243\n",
      "the loss: 10.8358669281 at step 244\n",
      "the loss: 11.9320812225 at step 245\n",
      "the loss: 12.1805057526 at step 246\n",
      "the loss: 5.6767578125 at step 247\n",
      "the loss: 10.5007820129 at step 248\n",
      "the loss: 11.8310375214 at step 249\n",
      "the loss: 10.6923923492 at step 250\n",
      "the loss: 8.28791999817 at step 251\n",
      "the loss: 7.31824064255 at step 252\n",
      "the loss: 10.4884538651 at step 253\n",
      "the loss: 11.0952548981 at step 254\n",
      "the loss: 4.47412919998 at step 255\n",
      "the loss: 10.3606367111 at step 256\n",
      "the loss: 13.763004303 at step 257\n",
      "the loss: 5.85851335526 at step 258\n",
      "the loss: 13.7405204773 at step 259\n",
      "the loss: 10.0558099747 at step 260\n",
      "the loss: 9.20642089844 at step 261\n",
      "the loss: 9.05260848999 at step 262\n",
      "the loss: 8.56772232056 at step 263\n",
      "the loss: 13.3327951431 at step 264\n",
      "the loss: 10.9359512329 at step 265\n",
      "the loss: 12.0392627716 at step 266\n",
      "the loss: 7.55101823807 at step 267\n",
      "the loss: 14.1620979309 at step 268\n",
      "the loss: 8.79326915741 at step 269\n",
      "the loss: 12.2811307907 at step 270\n",
      "the loss: 6.92282581329 at step 271\n",
      "the loss: 16.5837745667 at step 272\n",
      "the loss: 4.14961433411 at step 273\n",
      "the loss: 10.017660141 at step 274\n",
      "the loss: 10.9586486816 at step 275\n",
      "the loss: 6.94897174835 at step 276\n",
      "the loss: 13.3513851166 at step 277\n",
      "the loss: 15.7072257996 at step 278\n",
      "the loss: 11.9780387878 at step 279\n",
      "the loss: 14.420258522 at step 280\n",
      "the loss: 16.6835746765 at step 281\n",
      "the loss: 6.09320020676 at step 282\n",
      "the loss: 11.2917556763 at step 283\n",
      "the loss: 7.28602027893 at step 284\n",
      "the loss: 10.4347391129 at step 285\n",
      "the loss: 4.13903427124 at step 286\n",
      "the loss: 10.5379152298 at step 287\n",
      "the loss: 16.0098056793 at step 288\n",
      "the loss: 7.81423187256 at step 289\n",
      "the loss: 12.0849199295 at step 290\n",
      "the loss: 11.5078620911 at step 291\n",
      "the loss: 4.88664627075 at step 292\n",
      "the loss: 10.7287464142 at step 293\n",
      "the loss: 10.3450422287 at step 294\n",
      "the loss: 9.52201271057 at step 295\n",
      "the loss: 4.45937824249 at step 296\n",
      "the loss: 11.6213264465 at step 297\n",
      "the loss: 16.7350406647 at step 298\n",
      "the loss: 8.0951423645 at step 299\n",
      "the loss: 16.2100791931 at step 300\n",
      "the loss: 12.395693779 at step 301\n",
      "the loss: 12.1823968887 at step 302\n",
      "the loss: 4.45059490204 at step 303\n",
      "the loss: 12.1783027649 at step 304\n",
      "the loss: 7.73586177826 at step 305\n",
      "the loss: 12.6663780212 at step 306\n",
      "the loss: 16.8131904602 at step 307\n",
      "the loss: 12.3885822296 at step 308\n",
      "the loss: 8.8344745636 at step 309\n",
      "the loss: 9.51226902008 at step 310\n",
      "the loss: 7.95850467682 at step 311\n",
      "the loss: 7.40173149109 at step 312\n",
      "the loss: 10.979642868 at step 313\n",
      "the loss: 11.7586727142 at step 314\n",
      "the loss: 11.8216323853 at step 315\n",
      "the loss: 6.94903802872 at step 316\n",
      "the loss: 8.08603954315 at step 317\n",
      "the loss: 14.556397438 at step 318\n",
      "the loss: 9.42898654938 at step 319\n",
      "the loss: 15.3697271347 at step 320\n",
      "the loss: 9.32891464233 at step 321\n",
      "the loss: 15.5293617249 at step 322\n",
      "the loss: 16.4031677246 at step 323\n",
      "the loss: 10.3674068451 at step 324\n",
      "the loss: 8.14355659485 at step 325\n",
      "the loss: 13.2232437134 at step 326\n",
      "the loss: 5.77103424072 at step 327\n",
      "the loss: 12.8785409927 at step 328\n",
      "the loss: 12.0266819 at step 329\n",
      "the loss: 9.92369842529 at step 330\n",
      "the loss: 11.0496368408 at step 331\n",
      "the loss: 9.44511413574 at step 332\n",
      "the loss: 14.9467372894 at step 333\n",
      "the loss: 14.9593830109 at step 334\n",
      "the loss: 15.5313796997 at step 335\n",
      "the loss: 6.13433551788 at step 336\n",
      "the loss: 10.5434856415 at step 337\n",
      "the loss: 7.36542510986 at step 338\n",
      "the loss: 10.5261163712 at step 339\n",
      "the loss: 10.6214466095 at step 340\n",
      "the loss: 10.5832633972 at step 341\n",
      "the loss: 9.05854415894 at step 342\n",
      "the loss: 13.7842578888 at step 343\n",
      "the loss: 9.37975692749 at step 344\n",
      "the loss: 11.7696533203 at step 345\n",
      "the loss: 6.07929801941 at step 346\n",
      "the loss: 10.6338253021 at step 347\n",
      "the loss: 16.4906368256 at step 348\n",
      "the loss: 7.2361497879 at step 349\n",
      "the loss: 10.3044691086 at step 350\n",
      "the loss: 15.9383869171 at step 351\n",
      "the loss: 12.582611084 at step 352\n",
      "the loss: 12.6813659668 at step 353\n",
      "the loss: 7.91330528259 at step 354\n",
      "the loss: 11.2973957062 at step 355\n",
      "the loss: 8.49165153503 at step 356\n",
      "the loss: 8.38372612 at step 357\n",
      "the loss: 12.540345192 at step 358\n",
      "the loss: 14.6322669983 at step 359\n",
      "the loss: 12.7869052887 at step 360\n",
      "the loss: 11.1857156754 at step 361\n",
      "the loss: 13.9161643982 at step 362\n",
      "the loss: 10.8418674469 at step 363\n",
      "the loss: 13.5961551666 at step 364\n",
      "the loss: 11.1402330399 at step 365\n",
      "the loss: 14.3147697449 at step 366\n",
      "the loss: 14.8567676544 at step 367\n",
      "the loss: 9.04481220245 at step 368\n",
      "the loss: 10.0671329498 at step 369\n",
      "the loss: 13.437541008 at step 370\n",
      "the loss: 11.4864654541 at step 371\n",
      "the loss: 10.6810722351 at step 372\n",
      "the loss: 12.8096389771 at step 373\n",
      "the loss: 10.0746994019 at step 374\n",
      "the loss: 8.6390953064 at step 375\n",
      "the loss: 5.40186500549 at step 376\n",
      "the loss: 8.88957977295 at step 377\n",
      "the loss: 9.29899215698 at step 378\n",
      "the loss: 12.2257156372 at step 379\n",
      "the loss: 11.8948192596 at step 380\n",
      "the loss: 9.76097297668 at step 381\n",
      "the loss: 8.00214004517 at step 382\n",
      "the loss: 11.4010505676 at step 383\n",
      "the loss: 16.0731163025 at step 384\n",
      "the loss: 5.98764038086 at step 385\n",
      "the loss: 10.7813215256 at step 386\n",
      "the loss: 10.9103870392 at step 387\n",
      "the loss: 4.66086816788 at step 388\n",
      "the loss: 8.76645278931 at step 389\n",
      "the loss: 6.34362792969 at step 390\n",
      "the loss: 9.95084857941 at step 391\n",
      "the loss: 11.1854915619 at step 392\n",
      "the loss: 9.70069122314 at step 393\n",
      "the loss: 11.1420345306 at step 394\n",
      "the loss: 9.53414916992 at step 395\n",
      "the loss: 13.9852762222 at step 396\n",
      "the loss: 7.52695560455 at step 397\n",
      "the loss: 7.38112163544 at step 398\n",
      "the loss: 12.7595252991 at step 399\n",
      "the loss: 12.374502182 at step 400\n",
      "the loss: 9.36602973938 at step 401\n",
      "the loss: 16.6426353455 at step 402\n",
      "the loss: 10.3258857727 at step 403\n",
      "the loss: 7.37044525146 at step 404\n",
      "the loss: 16.6186714172 at step 405\n",
      "the loss: 11.6979637146 at step 406\n",
      "the loss: 15.9638538361 at step 407\n",
      "the loss: 10.7809810638 at step 408\n",
      "the loss: 4.85560512543 at step 409\n",
      "the loss: 8.56998252869 at step 410\n",
      "the loss: 5.13037109375 at step 411\n",
      "the loss: 11.7439289093 at step 412\n",
      "the loss: 9.3825044632 at step 413\n",
      "the loss: 9.9885635376 at step 414\n",
      "the loss: 9.27294635773 at step 415\n",
      "the loss: 10.7522220612 at step 416\n",
      "the loss: 6.84433984756 at step 417\n",
      "the loss: 6.97790908813 at step 418\n",
      "the loss: 10.350774765 at step 419\n",
      "the loss: 17.1213378906 at step 420\n",
      "the loss: 10.1723823547 at step 421\n",
      "the loss: 4.07967853546 at step 422\n",
      "the loss: 11.2273893356 at step 423\n",
      "the loss: 11.4965076447 at step 424\n",
      "the loss: 6.41204214096 at step 425\n",
      "the loss: 9.68052482605 at step 426\n",
      "the loss: 4.06915235519 at step 427\n",
      "the loss: 10.6171751022 at step 428\n",
      "the loss: 5.27963733673 at step 429\n",
      "the loss: 7.68407678604 at step 430\n",
      "the loss: 8.64165115356 at step 431\n",
      "the loss: 7.97276878357 at step 432\n",
      "the loss: 12.4127597809 at step 433\n",
      "the loss: 10.3465042114 at step 434\n",
      "the loss: 5.27794742584 at step 435\n",
      "the loss: 10.8927793503 at step 436\n",
      "the loss: 8.06760025024 at step 437\n",
      "the loss: 9.43441009521 at step 438\n",
      "the loss: 13.1399040222 at step 439\n",
      "the loss: 16.3001365662 at step 440\n",
      "the loss: 6.06646966934 at step 441\n",
      "the loss: 9.27238273621 at step 442\n",
      "the loss: 5.60225391388 at step 443\n",
      "the loss: 7.38068151474 at step 444\n",
      "the loss: 13.036901474 at step 445\n",
      "the loss: 15.5139160156 at step 446\n",
      "the loss: 7.41359138489 at step 447\n",
      "the loss: 11.2326126099 at step 448\n",
      "the loss: 10.7614593506 at step 449\n",
      "the loss: 7.34293937683 at step 450\n",
      "the loss: 16.0239315033 at step 451\n",
      "the loss: 8.77857494354 at step 452\n",
      "the loss: 7.20397281647 at step 453\n",
      "the loss: 6.46166276932 at step 454\n",
      "the loss: 8.90867996216 at step 455\n",
      "the loss: 17.5712966919 at step 456\n",
      "the loss: 15.4388380051 at step 457\n",
      "the loss: 10.9706401825 at step 458\n",
      "the loss: 11.9870443344 at step 459\n",
      "the loss: 14.5439510345 at step 460\n",
      "the loss: 9.77090454102 at step 461\n",
      "the loss: 12.8858699799 at step 462\n",
      "the loss: 7.00378656387 at step 463\n",
      "the loss: 10.5170946121 at step 464\n",
      "the loss: 13.2919092178 at step 465\n",
      "the loss: 13.8425159454 at step 466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-2986cfa86445>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#X_, y_ = model.cbow_batch_content()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#print X_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e1b3d00f3816>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbow_batch_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 _, loss = sess.run([self.train, self.loss], feed_dict={self.X: X, \n\u001b[0;32m---> 67\u001b[0;31m                                                                        self.y: y})\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m'the loss: {} at step {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matt.meng/.virtualenvs/kaggle/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matt.meng/.virtualenvs/kaggle/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matt.meng/.virtualenvs/kaggle/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/matt.meng/.virtualenvs/kaggle/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matt.meng/.virtualenvs/kaggle/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/matt.meng'\n",
    "pickle_file = 'processed_titles_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, pickle_file)) as input_file:\n",
    "    data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url', 'reverse_token_dict', 'pageViw', 'titles', 'token_dict']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = data['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[163, 2969, 20, 2805, 168],\n",
       " [27, 186, 3, 156, 210],\n",
       " [322, 1523, 16262, 31, 493],\n",
       " [54, 6, 181, 2939, 713],\n",
       " [49, 64, 635, 9, 1425]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cbow_data(titles, context_window):\n",
    "    span = 2 * context_window + 1\n",
    "    missing_count = 0\n",
    "    training_list = []\n",
    "    target_list = []\n",
    "    #X = np.zeros(shape=(batch_size, span-1), dtype=np.int32)\n",
    "    #y = np.zeros(shape=(batch_size, 1), dtype=np.int32)\n",
    "    for title in titles:\n",
    "        if len(title) < span:\n",
    "            missing_count += 1\n",
    "            continue\n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        buffer.extend(title[:span])\n",
    "        title_len = len(title)\n",
    "        for i in xrange(title_len-span+1):\n",
    "            buffer_list = list(buffer)\n",
    "            target_list.append([buffer_list.pop(context_window)])\n",
    "            training_list.append(buffer_list)\n",
    "            if i + span < title_len: \n",
    "                buffer.append(title[i+span])\n",
    "    print '{} short titles are passed by context_window {}'.format(missing_count, context_window)\n",
    "    return training_list, target_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 short titles are passed by context_window 2\n"
     ]
    }
   ],
   "source": [
    "training_list_, target_list_ = create_cbow_data(titles[:5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[163, 2969, 2805, 168],\n",
       " [27, 186, 156, 210],\n",
       " [322, 1523, 31, 493],\n",
       " [54, 6, 2939, 713],\n",
       " [49, 64, 9, 1425]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20], [3], [16262], [181], [635]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = [range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 short titles are passed by context_window 2\n"
     ]
    }
   ],
   "source": [
    "training_list_, target_list_ = create_cbow_data(title, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 3, 4],\n",
       " [1, 2, 4, 5],\n",
       " [2, 3, 5, 6],\n",
       " [3, 4, 6, 7],\n",
       " [4, 5, 7, 8],\n",
       " [5, 6, 8, 9]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [3], [4], [5], [6], [7]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 short titles are passed by context_window 2\n"
     ]
    }
   ],
   "source": [
    "training_list_, target_list_ = create_cbow_data(titles, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742529"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
