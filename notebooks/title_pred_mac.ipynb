{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import os, sys, multiprocessing, time\n",
    "import tensorflow as tf\n",
    "sys.path.append('/Users/matt.meng/dev/word2dev_model')\n",
    "from graph_model import word2vec\n",
    "from model_utils import create_local_model_path, create_local_log_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_word2vec_model(model_name):\n",
    "    \n",
    "    NUM_THREADS = 2*multiprocessing.cpu_count()-1\n",
    "    COMMON_PATH = os.path.join(os.path.expanduser(\"~\"), 'local_tensorflow_content')\n",
    "    \n",
    "    model_config = {}\n",
    "    model_config['model_name'] = model_name\n",
    "    model_config['restore_model'] = True\n",
    "    model_config['eval_mode'] = True\n",
    "\n",
    "    use_gpu = False\n",
    "    if use_gpu:\n",
    "        model_config['sess_config'] = tf.ConfigProto(log_device_placement=False,\n",
    "                                                     gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # the only way to completely not use GPU\n",
    "        model_config['sess_config'] = tf.ConfigProto(intra_op_parallelism_threads=NUM_THREADS)\n",
    "\n",
    "    model_config['model_path'] = create_local_model_path(COMMON_PATH, model_config['model_name'])\n",
    "    model_config['log_path'] = create_local_log_path(COMMON_PATH, model_config['model_name'])\n",
    "\n",
    "    model = word2vec(**model_config)\n",
    "    return model\n",
    "\n",
    "def collect_key_from_pickle_file(titles_pickle_file, title_key):\n",
    "    pickle_file_path = os.path.join(os.path.expanduser(\"~\"), titles_pickle_file)\n",
    "\n",
    "    with open(pickle_file_path, 'rb') as input_stream:\n",
    "        data = pickle.load(input_stream)\n",
    "    return data[title_key]\n",
    "\n",
    "\n",
    "def collect_multi_keys_from_pickle_file(titles_pickle_file, key_dict):\n",
    "    pickle_file_path = os.path.join(os.path.expanduser(\"~\"), titles_pickle_file)\n",
    "\n",
    "    with open(pickle_file_path, 'rb') as input_stream:\n",
    "        data = pickle.load(input_stream)\n",
    "        \n",
    "    content_dict = {}\n",
    "    for key in key_dict.keys():\n",
    "        content_dict[key] = data[key_dict[key]]\n",
    "    return content_dict\n",
    "\n",
    "\n",
    "class ProcessedTitle(object):\n",
    "    \n",
    "    def __init__(self, index_title, url, pageView):\n",
    "        self.index_title = index_title\n",
    "        self.url = url\n",
    "        self.pageView = pageView\n",
    "        title_array = map(ProcessedTitle.reverse_token_dict.get, self.index_title)\n",
    "        self.title = \" \".join(title_array) \n",
    "        \n",
    "    def create_word2vec_embeddings(self, word2vec_model):\n",
    "        max_vector, min_vector, mean_vector = word2vec_model.predict(self.index_title)\n",
    "        self.max_vector = max_vector\n",
    "        self.min_vector = min_vector\n",
    "        self.mean_vector = mean_vector\n",
    "        \n",
    "\n",
    "def create_title_dict_with_word2vec(content_dict, model):\n",
    "    processed_titles = []\n",
    "    cur_time = time.time()\n",
    "    fixed_couner = 1000\n",
    "    title_limit = 10000\n",
    "    ProcessedTitle.reverse_token_dict = content_dict['reverse_token_dict']\n",
    "    for i in xrange(title_limit):\n",
    "    #for i in xrange(len(content_dict['titles'])):\n",
    "        title = ProcessedTitle(index_title=content_dict['titles'][i], \n",
    "                               url=content_dict['url'][i], \n",
    "                               pageView=content_dict['pageView'][i])\n",
    "        title.create_word2vec_embeddings(model)\n",
    "        processed_titles.append(title)\n",
    "        if i != 0 and i % fixed_couner == 0:\n",
    "            print \"processing {} titles using {:.2f} seconds\".format(fixed_couner, time.time()-cur_time)\n",
    "            cur_time = time.time()\n",
    "    return processed_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles_pickle_file = 'lemmanized_no_stop_words_scrambled_titles.pkl'\n",
    "\n",
    "expected_keys = {\"titles\": 'titles', \"url\": 'url', 'pageView': \"pageViw\", 'reverse_token_dict': 'reverse_token_dict'}\n",
    "lemmatized_expected_keys = {\"titles\": 'target_titles', \"url\": 'url', 'pageView': \"pageViw\", 'reverse_token_dict': 'reverse_token_dict'}\n",
    "\n",
    "content_dict = collect_multi_keys_from_pickle_file(titles_pickle_file, lemmatized_expected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning, more than one model meta file is found in /Users/matt.meng/local_tensorflow_content/word2vec\n",
      "INFO:tensorflow:Restoring parameters from /Users/matt.meng/local_tensorflow_content/word2vec/models-1500\n",
      "restore trained models from /Users/matt.meng/local_tensorflow_content/word2vec\n",
      "restore model from step:  1500\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = build_word2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1000 titles using 29.10 seconds\n",
      "processing 1000 titles using 30.48 seconds\n",
      "processing 1000 titles using 27.58 seconds\n",
      "processing 1000 titles using 27.48 seconds\n",
      "processing 1000 titles using 29.54 seconds\n",
      "processing 1000 titles using 31.50 seconds\n",
      "processing 1000 titles using 36.84 seconds\n",
      "processing 1000 titles using 32.05 seconds\n",
      "processing 1000 titles using 29.59 seconds\n"
     ]
    }
   ],
   "source": [
    "processed_titles = create_title_dict_with_word2vec(content_dict, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_titles = sorted(processed_titles, key=lambda x: x.pageView, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump humiliate include john mccain\n",
      "[ -2.58553941e-02   1.22627921e-01   1.15935944e-01  -5.43628726e-03\n",
      "   1.08333386e-01   8.71216878e-02  -8.59051506e-05   1.53111294e-01\n",
      "  -6.82163984e-02   8.53824541e-02  -2.91144568e-02   1.24870621e-01\n",
      "   1.47591799e-01   1.33838326e-01  -7.23624676e-02  -6.17905147e-02\n",
      "  -3.55449095e-02  -6.23437613e-02  -1.95119996e-02   1.13402098e-01\n",
      "  -5.27736917e-02   1.35000676e-01   8.80233049e-02   9.40252393e-02\n",
      "   1.20826170e-01   5.60505837e-02   1.38159335e-01   8.52120072e-02\n",
      "   1.05636202e-01   1.23074263e-01  -2.69965511e-02   1.33987054e-01\n",
      "   1.21170469e-01   1.54107377e-01   1.18402325e-01   6.94712549e-02\n",
      "  -1.11002603e-03  -7.90434889e-03   1.48462862e-01   3.74956131e-02\n",
      "   8.38991776e-02   7.41552189e-02  -7.84530938e-02   9.15036188e-04\n",
      "   5.75002469e-02   1.00514486e-01   1.53303489e-01   1.29521772e-01\n",
      "   8.82143527e-02  -5.18808514e-03   1.20590270e-01   9.73786041e-02\n",
      "   1.26380906e-01   7.26668462e-02   8.70950967e-02   1.09721191e-01\n",
      "   1.35746539e-01   4.99214865e-02   1.47557124e-01   8.11043009e-02\n",
      "   9.22978669e-02   7.17865080e-02  -5.93796372e-02  -5.69407232e-02\n",
      "  -3.74307260e-02   9.03315023e-02   1.28768776e-02   1.38112873e-01\n",
      "   1.09561041e-01  -6.45380691e-02   1.63897783e-01  -3.43740778e-03\n",
      "  -7.34868050e-02  -2.03519128e-02   1.35448501e-01  -1.38148444e-03\n",
      "  -4.74289469e-02   1.34625897e-01  -7.15860203e-02  -5.35770245e-02\n",
      "   6.09673522e-02   4.27122526e-02   7.31566222e-03   1.09793201e-01\n",
      "  -4.35330980e-02   9.10743549e-02   6.62094355e-02  -6.37527928e-03\n",
      "   1.12890959e-01   1.97320625e-01  -7.23917112e-02  -9.65880230e-02\n",
      "   9.82858166e-02   6.47197887e-02  -2.56982353e-02   7.36873150e-02\n",
      "   1.19585253e-01   5.11333486e-03   1.01483054e-01  -6.17495701e-02\n",
      "   1.77686885e-01   8.58584344e-02   1.17015585e-01  -3.69084664e-02\n",
      "   5.70760183e-02   9.86268148e-02   1.23453029e-01   1.22363679e-01\n",
      "   9.83005464e-02   8.17060471e-02  -5.82638346e-02  -2.52730548e-02\n",
      "   2.17367932e-02   1.03913635e-01   1.55344188e-01  -6.58359453e-02\n",
      "  -1.28990039e-02   8.33300576e-02   9.75302532e-02  -2.63824943e-03\n",
      "   1.60793722e-01   1.18428238e-01  -1.06632551e-02   4.90935482e-02\n",
      "   1.17410878e-02   3.41705084e-02  -5.10501005e-02  -3.97078134e-02]\n"
     ]
    }
   ],
   "source": [
    "index = 10\n",
    "print sorted_titles[index].title\n",
    "print sorted_titles[index].max_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_titles[2].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles_pickle_file = 'lemmanized_no_stop_words_processed_titles.pkl'\n",
    "pickle_file_path = os.path.join(os.path.expanduser(\"~\"), titles_pickle_file)\n",
    "\n",
    "with open(pickle_file_path, 'rb') as input_stream:\n",
    "    data = pickle.load(input_stream)\n",
    "\n",
    "print data.keys()\n",
    "titles = data['titles']\n",
    "reverse_token_dict = data['reverse_token_dict']\n",
    "title_urls = data['url']\n",
    "title_pageViews = data['pageViw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### work on the scrambled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles_pickle_file = 'lemmanized_no_stop_words_scrambled_titles.pkl'\n",
    "pickle_file_path = os.path.join(os.path.expanduser(\"~\"), titles_pickle_file)\n",
    "\n",
    "with open(pickle_file_path, 'rb') as input_stream:\n",
    "    tmp_data = pickle.load(input_stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tmp_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(tmp_data['target_titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_data['target_titles'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
