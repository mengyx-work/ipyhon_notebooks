{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, time, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "sys.path.append('/home/ymm/kaggle/xgboost_hyperopt')\n",
    "import utils.bosch_functions as bosch_functions\n",
    "from utils.wrapped_xgboost import xgboost_classifier\n",
    "from utils.validation_tools import score_MCC, MCC, create_validation_index\n",
    "from utils.models import CombinedModel\n",
    "from utils.data_munge import remove_single_value_columns\n",
    "from utils.feature_engineering import NumericalFeatureEngineering, getRelativeTimeColumns, BasicDate_FeatureEngineering\n",
    "from utils.feature_engineering import getTimeChangeColumns, getTimeSteps, build_IndexFeatures\n",
    "\n",
    "data_path = '/home/ymm/bosch/'\n",
    "\n",
    "train_num_file   = 'train_numeric.csv'\n",
    "train_cat_file   = 'train_categorical.csv'\n",
    "train_date_file  = 'train_date.csv'\n",
    "test_num_file    = 'test_numeric.csv'\n",
    "test_cat_file    = 'test_categorical.csv'\n",
    "test_date_file   = 'test_date.csv'\n",
    "\n",
    "sample_submission_file   = 'sample_submission.csv'\n",
    "\n",
    "start_time_column_name = 'L0_S0_D1'\n",
    "id_column_name = 'Id'\n",
    "dep_var_name = 'Response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#id_df = pd.read_csv(join(data_path, train_num_file),    index_col='Id', usecols=['Id'])\n",
    "#id_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot_row_num = 1183747\n",
    "num_rows = 150000\n",
    "skip = sorted(random.sample(xrange(1,tot_row_num + 1),tot_row_num - num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading date using 117.0 seconds\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "start_time = time.time()\n",
    "## loading the data by using the skipped_row_num list\n",
    "\n",
    "#train_num = pd.read_csv(join(data_path, train_num_file),    index_col='Id', nrows=num_rows)\n",
    "#train_dat = pd.read_csv(join(data_path, train_date_file),   index_col='Id', nrows=num_rows)\n",
    "#train_cat = pd.read_csv(join(data_path, train_cat_file),    index_col='Id', nrows=num_rows)\n",
    "\n",
    "## randomly select certain rows\n",
    "train_num = pd.read_csv(join(data_path, train_num_file),    index_col='Id', skiprows=skip, nrows=num_rows)\n",
    "train_dat = pd.read_csv(join(data_path, train_date_file),   index_col='Id', skiprows=skip, nrows=num_rows)\n",
    "train_cat = pd.read_csv(join(data_path, train_cat_file),    index_col='Id', skiprows=skip, nrows=num_rows)\n",
    "\n",
    "'''\n",
    "test_num = pd.read_csv(join(data_path, test_num_file),      index_col='Id', nrows=num_rows)\n",
    "test_dat = pd.read_csv(join(data_path, test_date_file),     index_col='Id', nrows=num_rows)\n",
    "test_cat = pd.read_csv(join(data_path, test_cat_file),      index_col='Id', nrows=num_rows)\n",
    "'''\n",
    "\n",
    "print 'finish loading date using {} seconds'.format(round(time.time() - start_time, 0))\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 969) (150000, 1156)\n"
     ]
    }
   ],
   "source": [
    "print train_num.shape, train_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0_S0_D1</th>\n",
       "      <th>L0_S0_D3</th>\n",
       "      <th>L0_S0_D5</th>\n",
       "      <th>L0_S0_D7</th>\n",
       "      <th>L0_S0_D9</th>\n",
       "      <th>L0_S0_D11</th>\n",
       "      <th>L0_S0_D13</th>\n",
       "      <th>L0_S0_D15</th>\n",
       "      <th>L0_S0_D17</th>\n",
       "      <th>L0_S0_D19</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S50_D4246</th>\n",
       "      <th>L3_S50_D4248</th>\n",
       "      <th>L3_S50_D4250</th>\n",
       "      <th>L3_S50_D4252</th>\n",
       "      <th>L3_S50_D4254</th>\n",
       "      <th>L3_S51_D4255</th>\n",
       "      <th>L3_S51_D4257</th>\n",
       "      <th>L3_S51_D4259</th>\n",
       "      <th>L3_S51_D4261</th>\n",
       "      <th>L3_S51_D4263</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>1104.78</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>1633.80</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>1674.90</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>55.37</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>591.90</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     L0_S0_D1  L0_S0_D3  L0_S0_D5  L0_S0_D7  L0_S0_D9  L0_S0_D11  L0_S0_D13  \\\n",
       "Id                                                                            \n",
       "26    1104.78   1104.78   1104.78   1104.78   1104.78    1104.78    1104.78   \n",
       "38    1633.80   1633.80   1633.80   1633.80   1633.80    1633.80    1633.80   \n",
       "82    1674.90   1674.90   1674.90   1674.90   1674.90    1674.90    1674.90   \n",
       "122     55.37     55.37     55.37     55.37     55.37      55.37      55.37   \n",
       "123    591.90    591.90    591.90    591.90    591.90     591.90     591.90   \n",
       "\n",
       "     L0_S0_D15  L0_S0_D17  L0_S0_D19      ...       L3_S50_D4246  \\\n",
       "Id                                        ...                      \n",
       "26     1104.78    1104.78    1104.78      ...                NaN   \n",
       "38     1633.80    1633.80    1633.80      ...                NaN   \n",
       "82     1674.90    1674.90    1674.90      ...                NaN   \n",
       "122      55.37      55.37      55.37      ...                NaN   \n",
       "123     591.90     591.90     591.90      ...                NaN   \n",
       "\n",
       "     L3_S50_D4248  L3_S50_D4250  L3_S50_D4252  L3_S50_D4254  L3_S51_D4255  \\\n",
       "Id                                                                          \n",
       "26            NaN           NaN           NaN           NaN           NaN   \n",
       "38            NaN           NaN           NaN           NaN           NaN   \n",
       "82            NaN           NaN           NaN           NaN           NaN   \n",
       "122           NaN           NaN           NaN           NaN           NaN   \n",
       "123           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "     L3_S51_D4257  L3_S51_D4259  L3_S51_D4261  L3_S51_D4263  \n",
       "Id                                                           \n",
       "26            NaN           NaN           NaN           NaN  \n",
       "38            NaN           NaN           NaN           NaN  \n",
       "82            NaN           NaN           NaN           NaN  \n",
       "122           NaN           NaN           NaN           NaN  \n",
       "123           NaN           NaN           NaN           NaN  \n",
       "\n",
       "[5 rows x 1156 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-27e185460d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mremove_single_value_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Response'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mremove_single_value_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mremove_single_value_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_num' is not defined"
     ]
    }
   ],
   "source": [
    "remove_single_value_columns(train_num, 'Response', test=test_num)\n",
    "remove_single_value_columns(train_dat, test=test_dat)\n",
    "remove_single_value_columns(train_cat, test=test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbin_num = 1 ## number of bins to separate data by start_time\\ntmp_train, tmp_test, bins, bin_names = bosch_functions.create_grouped_index_df(bin_num)\\n#none_selected_window_num = ['0']\\nnone_selected_window_num = [np.NaN]\\nskipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\\nskipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\\n\\n\\nnum_rows = 50000\\nstart_time = time.time()\\n## loading the data by using the skipped_row_num list\\ntrain_num = pd.read_csv(join(data_path, train_num_file),    index_col='Id',  skiprows=skipped_train_row_num, nrows=num_rows)\\ntrain_dat = pd.read_csv(join(data_path, train_date_file),   index_col='Id',  skiprows=skipped_train_row_num, nrows=num_rows)\\ntest_num = pd.read_csv(join(data_path, test_num_file),      index_col='Id',  skiprows=skipped_test_row_num,  nrows=num_rows)\\ntest_dat = pd.read_csv(join(data_path, test_date_file),     index_col='Id',  skiprows=skipped_test_row_num,  nrows=num_rows)\\n\\nprint 'finish loading date using {} seconds'.format(round(time.time() - start_time, 0))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bin_num = 1 ## number of bins to separate data by start_time\n",
    "tmp_train, tmp_test, bins, bin_names = bosch_functions.create_grouped_index_df(bin_num)\n",
    "#none_selected_window_num = ['0']\n",
    "none_selected_window_num = [np.NaN]\n",
    "skipped_test_row_num = tmp_test.loc[tmp_test['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "skipped_train_row_num = tmp_train.loc[tmp_train['time_window_num'].isin(none_selected_window_num), 'row_num'].tolist()\n",
    "\n",
    "\n",
    "num_rows = 50000\n",
    "start_time = time.time()\n",
    "## loading the data by using the skipped_row_num list\n",
    "train_num = pd.read_csv(join(data_path, train_num_file),    index_col='Id',  skiprows=skipped_train_row_num, nrows=num_rows)\n",
    "train_dat = pd.read_csv(join(data_path, train_date_file),   index_col='Id',  skiprows=skipped_train_row_num, nrows=num_rows)\n",
    "test_num = pd.read_csv(join(data_path, test_num_file),      index_col='Id',  skiprows=skipped_test_row_num,  nrows=num_rows)\n",
    "test_dat = pd.read_csv(join(data_path, test_date_file),     index_col='Id',  skiprows=skipped_test_row_num,  nrows=num_rows)\n",
    "\n",
    "print 'finish loading date using {} seconds'.format(round(time.time() - start_time, 0))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### section to build station-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_columns = train_dat.columns.tolist()\n",
    "num_columns = train_num.columns.tolist()\n",
    "num_columns.remove(dep_var_name)\n",
    "\n",
    "def build_column_dict(columns):\n",
    "    col_dict = {}\n",
    "    for col in columns:\n",
    "        tmpList = col.split('_')[0:2]\n",
    "        key = ('_').join(tmpList)\n",
    "        if key not in col_dict:\n",
    "            col_dict[key] = [col]\n",
    "        else:\n",
    "            col_dict[key].append(col)\n",
    "            \n",
    "    return col_dict\n",
    "\n",
    "\n",
    "def build_station_features(df, col_dict, prefix='dat'):\n",
    "    features = pd.DataFrame()\n",
    "    for key, value in col_dict.items():\n",
    "        features['{}_{}_{}'.format(prefix, key, 'mean')] = df[value].mean(axis=1)\n",
    "        features['{}_{}_{}'.format(prefix, key, 'max')] = df[value].max(axis=1)\n",
    "        features['{}_{}_{}'.format(prefix, key, 'min')] = df[value].min(axis=1)\n",
    "        features['{}_{}_{}'.format(prefix, key, 'var')] = df[value].var(axis=1)\n",
    "    return features\n",
    "    \n",
    "    \n",
    "dat_col_dict = build_column_dict(dat_columns)\n",
    "num_col_dict = build_column_dict(num_columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish feature engineering date station using 0.93 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_dat_stations = build_station_features(train_dat, dat_col_dict, 'dat')\n",
    "test_dat_stations = build_station_features(test_dat, dat_col_dict, 'dat')\n",
    "\n",
    "train_num_stations = build_station_features(train_num, num_col_dict, 'num')\n",
    "test_num_stations = build_station_features(test_num, num_col_dict, 'num')\n",
    "\n",
    "print 'finish feature engineering date station using {} minutes'.format(round((time.time() - start_time)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BasicCat_FeatureEngineering(train_cat):\n",
    "    ## feature engineering on the date features\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    column_names = train_cat.columns.tolist()\n",
    "    column_names.append('NaN')\n",
    "    encoder.fit(column_names)\n",
    "    dat_new_fea = pd.DataFrame()\n",
    "    dat_new_fea['cat_sum'] = train_cat.sum(axis=1)\n",
    "    dat_new_fea['cat_mean'] = train_cat.mean(axis=1)\n",
    "    dat_new_fea['cat_nan_count'] = train_cat.isnull().sum(axis=1)\n",
    "    dat_new_fea['cat_max'] = train_cat.max(axis=1)\n",
    "    dat_new_fea['cat_min'] = train_cat.min(axis=1)\n",
    "    dat_new_fea['cat_max_min_diff'] = dat_new_fea['cat_max'] - dat_new_fea['cat_min']\n",
    "    dat_new_fea['cat_max_min_ratio'] = dat_new_fea['cat_min'] / dat_new_fea['cat_max']\n",
    "\n",
    "    dat_new_fea['cat_idxmax'] = train_cat.idxmax(axis=1)\n",
    "    dat_new_fea['cat_idxmax'].fillna('NaN', inplace=True)\n",
    "    dat_new_fea['cat_idxmax'] = encoder.transform(dat_new_fea['cat_idxmax'])\n",
    "    dat_new_fea['cat_idxmin'] = train_cat.idxmin(axis=1)\n",
    "    dat_new_fea['cat_idxmin'].fillna('NaN', inplace=True)\n",
    "    dat_new_fea['cat_idxmin'] = encoder.transform(dat_new_fea['cat_idxmin'])\n",
    "    return dat_new_fea\n",
    "\n",
    "\n",
    "\n",
    "def encode_categorical_by_dep_var(train, test, dep_var_column='Response'):\n",
    "    for col_name in train.columns:\n",
    "        if col_name == dep_var_column:\n",
    "            continue\n",
    "        dep_var_mean = train[[col_name, dep_var_column]].groupby(col_name).mean()\n",
    "    \n",
    "        dep_var_dict = {}\n",
    "        for level in dep_var_mean.index.tolist():\n",
    "            dep_var_dict[level] = dep_var_mean.ix[level, dep_var_column]\n",
    "    \n",
    "        train[col_name] = train[col_name].replace(dep_var_dict)  \n",
    "        test[col_name] = test[col_name].replace(dep_var_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish generating categorical features using 93.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_cat['Response'] = train_num['Response']\n",
    "encode_categorical_by_dep_var(train_cat, test_cat)\n",
    "train_cat.drop('Response', axis=1, inplace=True)\n",
    "\n",
    "train_cat_Basics = BasicCat_FeatureEngineering(train_cat)\n",
    "test_cat_Basics  = BasicCat_FeatureEngineering(train_cat)\n",
    "\n",
    "print 'finish generating categorical features using {} seconds'.format(round(time.time() - start_time, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 1730) (150000, 1730)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0_S1_F25</th>\n",
       "      <th>L0_S1_F27</th>\n",
       "      <th>L0_S1_F29</th>\n",
       "      <th>L0_S1_F31</th>\n",
       "      <th>L0_S2_F33</th>\n",
       "      <th>L0_S2_F35</th>\n",
       "      <th>L0_S2_F37</th>\n",
       "      <th>L0_S2_F39</th>\n",
       "      <th>L0_S2_F41</th>\n",
       "      <th>L0_S2_F43</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S49_F4225</th>\n",
       "      <th>L3_S49_F4227</th>\n",
       "      <th>L3_S49_F4229</th>\n",
       "      <th>L3_S49_F4230</th>\n",
       "      <th>L3_S49_F4232</th>\n",
       "      <th>L3_S49_F4234</th>\n",
       "      <th>L3_S49_F4235</th>\n",
       "      <th>L3_S49_F4237</th>\n",
       "      <th>L3_S49_F4239</th>\n",
       "      <th>L3_S49_F4240</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1730 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L0_S1_F25  L0_S1_F27  L0_S1_F29  L0_S1_F31  L0_S2_F33  L0_S2_F35  \\\n",
       "Id                                                                     \n",
       "38        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "71        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "73        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "75        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "93        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "    L0_S2_F37  L0_S2_F39  L0_S2_F41  L0_S2_F43      ...       L3_S49_F4225  \\\n",
       "Id                                                  ...                      \n",
       "38        NaN        NaN        NaN        NaN      ...                NaN   \n",
       "71        NaN        NaN        NaN        NaN      ...                NaN   \n",
       "73        NaN        NaN        NaN        NaN      ...                NaN   \n",
       "75        NaN        NaN        NaN        NaN      ...                NaN   \n",
       "93        NaN        NaN        NaN        NaN      ...                NaN   \n",
       "\n",
       "    L3_S49_F4227  L3_S49_F4229  L3_S49_F4230  L3_S49_F4232  L3_S49_F4234  \\\n",
       "Id                                                                         \n",
       "38           NaN           NaN           NaN           NaN           NaN   \n",
       "71           NaN           NaN           NaN           NaN           NaN   \n",
       "73           NaN           NaN           NaN           NaN           NaN   \n",
       "75           NaN           NaN           NaN           NaN           NaN   \n",
       "93           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    L3_S49_F4235  L3_S49_F4237  L3_S49_F4239  L3_S49_F4240  \n",
       "Id                                                          \n",
       "38           NaN           NaN           NaN           NaN  \n",
       "71           NaN           NaN           NaN           NaN  \n",
       "73           NaN           NaN           NaN           NaN  \n",
       "75           NaN           NaN           NaN           NaN  \n",
       "93           NaN           NaN           NaN           NaN  \n",
       "\n",
       "[5 rows x 1730 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train_cat.shape, test_cat.shape\n",
    "train_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish feature engineering date using 2.64 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "combined_train_cat = pd.concat([train_cat, train_cat_Basics], axis=1)\n",
    "combined_test_cat  = pd.concat([test_cat, test_cat_Basics], axis=1)                                                                                                                                                 \n",
    "print 'finish feature engineering date using {} seconds'.format(round((time.time() - start_time), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### numerical feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined train numerical feature shape: (150000, 978), combined test numerical features shape: (150000, 977)\n"
     ]
    }
   ],
   "source": [
    "#### numerical feature engineering work\n",
    "train_num_Basics = NumericalFeatureEngineering(train_num)\n",
    "test_num_Basics = NumericalFeatureEngineering(test_num)\n",
    "\n",
    "combined_train_num = pd.concat([train_num, train_num_Basics], axis=1)\n",
    "combined_test_num  = pd.concat([test_num, test_num_Basics], axis=1)                                                                            \n",
    "print 'combined train numerical feature shape: {}, combined test numerical features shape: {}'.format(combined_train_num.shape, combined_test_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### section of date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## basic features from tmp_train_dat\n",
    "train_dat_Basics = BasicDate_FeatureEngineering(train_dat)\n",
    "test_dat_Basics  = BasicDate_FeatureEngineering(test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train data dimension:  (150000, 1146)\n",
      "raw test data dimension:  (150000, 1146)\n",
      "processed train data dimension:  (150000, 969)\n",
      "processed test data dimension:  (150000, 969)\n"
     ]
    }
   ],
   "source": [
    "## normalized date columns\n",
    "train_dat_Norm = train_dat.apply(getRelativeTimeColumns, axis=1)\n",
    "test_dat_Norm  = test_dat.apply(getRelativeTimeColumns, axis=1)\n",
    "## remove single-valued columns\n",
    "remove_single_value_columns(train_dat_Norm, test=test_dat_Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "column_names = train_dat.columns.tolist()\n",
    "column_names.append('NaN')\n",
    "encoder.fit(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TimeDiff features\n",
    "train_dat_TimeDiff = train_dat.apply(getTimeChangeColumns, axis=1)\n",
    "test_dat_TimeDiff  = test_dat.apply(getTimeChangeColumns, axis=1)\n",
    "TimeDiff_ColumnNames = ['time_diff_start_col', 'time_diff_end_col', 'time_diff_value',\n",
    "                        'time_ratio_value', 'first_time_value', 'last_time_value', 'first_date_value']\n",
    "train_dat_TimeDiff.columns = TimeDiff_ColumnNames\n",
    "test_dat_TimeDiff.columns = TimeDiff_ColumnNames\n",
    "\n",
    "for column in ['time_diff_start_col', 'time_diff_end_col']:\n",
    "    train_dat_TimeDiff[column].fillna('NaN', inplace=True)\n",
    "    train_dat_TimeDiff[column] = encoder.transform(train_dat_TimeDiff[column])\n",
    "    \n",
    "    test_dat_TimeDiff[column].fillna('NaN', inplace=True)\n",
    "    test_dat_TimeDiff[column] = encoder.transform(test_dat_TimeDiff[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "## section to create timeStep features\n",
    "\n",
    "unique_value_counts = 6\n",
    "timeStep_columnNames = []\n",
    "column_name_columns = []\n",
    "for i in xrange(unique_value_counts):\n",
    "    timeStep_columnNames.extend(['time_diff_step_{}'.format(i), 'column_counts_step_{}'.format(i),\n",
    "                                 'time_cost_step_{}'.format(i), 'first_column_step_{}'.format(i)])\n",
    "    column_name_columns.append('first_column_step_{}'.format(i))\n",
    "\n",
    "train_dat_TimeStep = train_dat_Norm.apply(getTimeSteps, axis=1)\n",
    "test_dat_TimeStep  = test_dat_Norm.apply(getTimeSteps, axis=1)\n",
    "train_dat_TimeStep.columns = timeStep_columnNames\n",
    "test_dat_TimeStep.columns  = timeStep_columnNames\n",
    "\n",
    "for column in column_name_columns:\n",
    "    train_dat_TimeStep[column].fillna('NaN', inplace=True)\n",
    "    test_dat_TimeStep[column].fillna('NaN', inplace=True)\n",
    "    train_dat_TimeStep[column] = encoder.transform(train_dat_TimeStep[column])\n",
    "    test_dat_TimeStep[column] = encoder.transform(test_dat_TimeStep[column])\n",
    "\n",
    "\n",
    "print 'finish generating TimeStep features using {} seconds'.format(round(time.time() - start_time, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "combined_train_dat = pd.concat([train_dat_Norm, train_dat_Basics, train_dat_TimeDiff, train_dat_TimeStep], axis=1)\n",
    "combined_test_dat  = pd.concat([test_dat_Norm, test_dat_Basics, test_dat_TimeDiff, test_dat_TimeStep], axis=1)                                                                                                                                                 \n",
    "print 'finish feature engineering date using {} minutes'.format(round((time.time() - start_time)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print combined_train_dat.shape, combined_test_dat.shape\n",
    "combined_train_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_datIndex_features = build_IndexFeatures(combined_train_dat, combined_test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the feature importances from xgboost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_feature_imp = pd.read_csv('/home/ymm/full_data_xgb_feature_importance.csv', index_col='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_feature_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## sort by the norm_fscore_sum\n",
    "sorted_combined_imp = xgb_feature_imp.sort_values(by=['norm_fscore_sum'], ascending=False)\n",
    "imp_feature = sorted_combined_imp.index[sorted_combined_imp['norm_fscore_sum'] >= 0.005].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_important_features(df, imp_feature, test_df = None, dep_var_name = 'Response'):\n",
    "    imp_col_names = [col for col in df.columns if col in imp_feature]\n",
    "    print 'total {} columns in original DataFrame, select {} columns'.format(df.shape[1], len(imp_col_names))\n",
    "    train_col_names = imp_col_names[:]\n",
    "    test_col_names = imp_col_names[:]\n",
    "    if dep_var_name in df.columns:    \n",
    "        train_col_names.append(dep_var_name)\n",
    "    if test_df is None:\n",
    "        return df[train_col_names]\n",
    "    else:\n",
    "        return df[train_col_names], test_df[test_col_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_train_num, selected_test_num  = select_important_features(combined_train_num, imp_feature, combined_test_num)\n",
    "selected_train_dat, selected_test_dat  = select_important_features(combined_train_dat, imp_feature, combined_test_dat)\n",
    "selected_train_idx = select_important_features(train_test_datIndex_features, imp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print selected_train_num.shape, selected_test_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## combined data with categorical features\n",
    "combined_train = pd.concat([selected_train_num, selected_train_dat, selected_train_idx.ix[combined_train_num.index, :]], axis=1)\n",
    "combined_test  = pd.concat([selected_test_num,  selected_test_dat,  selected_train_idx.ix[combined_test_num.index, :]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine all the features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combined_train = pd.concat([train_dat_stations, train_num_stations, combined_train_num, combined_train_dat, train_test_datIndex_features.ix[combined_train_num.index, :]], axis=1)\n",
    "#combined_test  = pd.concat([test_dat_stations, test_num_stations, combined_test_num,  combined_test_dat,  train_test_datIndex_features.ix[combined_test_num.index, :]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combined_train = pd.concat([combined_train_num, combined_train_dat, train_test_datIndex_features.ix[combined_train_num.index, :]], axis=1)\n",
    "#combined_test  = pd.concat([combined_test_num,  combined_test_dat,  train_test_datIndex_features.ix[combined_test_num.index, :]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## combined data with categorical features\n",
    "#combined_train = pd.concat([combined_train_cat, combined_train_num, combined_train_dat, train_test_datIndex_features.ix[combined_train_num.index, :]], axis=1)\n",
    "#combined_test  = pd.concat([combined_test_cat,  combined_test_num,  combined_test_dat,  train_test_datIndex_features.ix[combined_test_num.index, :]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print combined_test.shape\n",
    "combined_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print combined_train.shape\n",
    "combined_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"eta\"]                      = 0.0075\n",
    "params[\"subsample\"]                = 0.8\n",
    "params[\"colsample_bytree\"]         = 0.8\n",
    "params[\"num_round\"]                = 501\n",
    "params[\"max_depth\"]                = 5\n",
    "params[\"gamma\"]                    = 0\n",
    "params[\"metrics\"]                  = 'auc'\n",
    "params['eval_metric']              = 'auc'\n",
    "params[\"seed\"]                     = 999\n",
    "params['verbose_eval']             = 50\n",
    "## whether to use weights\n",
    "params['use_base_score']           = True\n",
    "params['use_weights']              = True\n",
    "#params['use_scale_pos_weight']     = True\n",
    "params[\"val\"]                      = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "skf = StratifiedKFold(combined_train[dep_var_name], 4)\n",
    "\n",
    "for train_index, valid_index in skf:\n",
    "    valid_data = combined_train.iloc[valid_index]\n",
    "    tmp_train  = combined_train.iloc[train_index]\n",
    "\n",
    "    y = tmp_train[dep_var_name].values\n",
    "    X = tmp_train.drop(dep_var_name, axis=1)\n",
    "\n",
    "    valid_y = valid_data[dep_var_name].values\n",
    "    valid_X = valid_data.drop(dep_var_name, axis=1)\n",
    "    \n",
    "    model = xgboost_classifier(label_name = dep_var_name, params = params, model_file='test_bosch_xgb_model')\n",
    "    model.fit(tmp_train, dep_var_name)\n",
    "    \n",
    "    pred = model.predict(valid_X)\n",
    "    print 'result from using constant fraction: \\n', score_MCC(valid_y, pred)\n",
    "    print '\\n'\n",
    "    print 'result from using flexsible threshold:', CombinedModel.mcc_eval_func(valid_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## Section of regular validation #######################\n",
    "train_index, valid_index = create_validation_index(combined_train, 0.3, dep_var_name, True)\n",
    "valid_data = combined_train.ix[valid_index]\n",
    "tmp_train  = combined_train.ix[train_index]\n",
    "\n",
    "y = tmp_train[dep_var_name].values\n",
    "X = tmp_train.drop(dep_var_name, axis=1)\n",
    "\n",
    "valid_y = valid_data[dep_var_name].values\n",
    "valid_X = valid_data.drop(dep_var_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"eta\"]                      = 0.0075\n",
    "params[\"subsample\"]                = 0.8\n",
    "params[\"colsample_bytree\"]         = 0.8\n",
    "params[\"num_round\"]                = 501\n",
    "params[\"max_depth\"]                = 5\n",
    "params[\"gamma\"]                    = 0\n",
    "params[\"metrics\"]                  = 'auc'\n",
    "params['eval_metric']              = 'auc'\n",
    "params[\"seed\"]                     = 999\n",
    "params['verbose_eval']             = 50\n",
    "## whether to use weights\n",
    "params['use_base_score']           = True\n",
    "params['use_weights']              = True\n",
    "#params['use_scale_pos_weight']     = True\n",
    "params[\"val\"]                      = False\n",
    "\n",
    "model = xgboost_classifier(label_name = dep_var_name, params = params, model_file='bosch_xgb_model')\n",
    "model.fit(tmp_train, dep_var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(valid_X)\n",
    "\n",
    "print 'result from using constant fraction: \\n', score_MCC(valid_y, pred)\n",
    "print '\\n'\n",
    "print 'result from using flexsible threshold:', CombinedModel.mcc_eval_func(valid_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############## Section of regular validation #######################\n",
    "train_index, valid_index = create_validation_index(combined_train, 0.3, dep_var_name, True)\n",
    "valid_data = combined_train.ix[valid_index]\n",
    "tmp_train  = combined_train.ix[train_index]\n",
    "\n",
    "y = tmp_train[dep_var_name].values\n",
    "X = tmp_train.drop(dep_var_name, axis=1)\n",
    "\n",
    "valid_y = valid_data[dep_var_name].values\n",
    "valid_X = valid_data.drop(dep_var_name, axis=1)\n",
    "\n",
    "model.fit(tmp_train, dep_var_name)\n",
    "pred = model.predict(valid_X)\n",
    "\n",
    "print 'result from using constant fraction: \\n', score_MCC(valid_y, pred)\n",
    "print '\\n'\n",
    "print 'result from using flexsible threshold:', CombinedModel.mcc_eval_func(valid_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############## Section of regular validation #######################\n",
    "train_index, valid_index = create_validation_index(combined_train, 0.3, dep_var_name, True)\n",
    "valid_data = combined_train.ix[valid_index]\n",
    "tmp_train  = combined_train.ix[train_index]\n",
    "\n",
    "y = tmp_train[dep_var_name].values\n",
    "X = tmp_train.drop(dep_var_name, axis=1)\n",
    "\n",
    "valid_y = valid_data[dep_var_name].values\n",
    "valid_X = valid_data.drop(dep_var_name, axis=1)\n",
    "\n",
    "model.fit(tmp_train, dep_var_name)\n",
    "pred = model.predict(valid_X)\n",
    "\n",
    "print 'result from using constant fraction: \\n', score_MCC(valid_y, pred)\n",
    "print '\\n'\n",
    "print 'result from using flexsible threshold:', CombinedModel.mcc_eval_func(valid_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
